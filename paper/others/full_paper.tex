\documentclass{article}

\input{commands}

\title{Derivative Free Model-Based Methods for Local Constrained Optimization}
\author{Trever Hallock}
%\date{Comments from Steve Billups, June 29, 2018}
% Remove the % from the previous line and change the date if you want a particular date to be displayed; otherwise, today's date is displayed by default.

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\let\oldref\ref
\renewcommand{\ref}[1]{(\oldref{#1})}

\begin{document}

\maketitle

\begin{abstract}

We propose a model-based trust-region algorithm for constrained optimization problems with linear constraints in which derivatives of the objective function are not available and the objective function values outside the feasible region are not available.
In each iteration, the objective function is approximated by an interpolation model, which is then minimized over a trust region.
To ensure feasibility of all iterates, an ellipsoidal trust region is constructed at each iterate that is contained within the feasible region.
We explore several choices of trust region and compare them with different functions, and types of constraints.
Because the constraints are included within the trust region, all iterates as well as sample points are feasible.
We work within the pyomo framework and test using ipopt optimization routines. We provide global convergence analysis thoughts, some numerical results, and suggest how we will extend this to include more general constraints.

\end{abstract}

\newpage

\tableofcontents

\newpage

%\section{Where to go}
%
%want decrease the radius faster when we are critical than when we are innacurate
%
%we decrease the radius when the step size is small compared to the trust region radius
%
%I have thoughts about another strategy to decrease the radius faster
%
%use vertices of the constraints: 
%
%\begin{verbatim}
%So that I don't forget it:
%https://www.mn.uio.no/math/english/people/aca/michaelf/papers/gradbdswach3d.pdf
%\end{verbatim}
%
%Kriging seems appealing because we can add more points without hurting the poisedness.
%\begin{verbatim}
%https://www.tandfonline.com/doi/abs/10.1080/0305215X.2015.1082350?src=recsys&journalCode=geno20
%\end{verbatim}

\input{shared}



\subsubsection{Hueristically Good Ellipsoid}

It may be possible to encourage the ellipse along the desired direction while still maintaining a large enough volume if we include information from the current model functions.
Among the desirable traits for the new ellipse, we would like:
\begin{itemize}
    \item The ellipse should not only have a large volume, but that volume should be distributed where the next iterate is likely to lie
    \item The ellipse should reuse as many sample points from the previous iterate as possible.
\end{itemize}


%By Lemma $3.7$ of \cite{DUMMY:intro_book} pp. 45 we know that the model will be poised for any subset.

In order to address the first item, we developed a hueristic to increase $\mathbb P(s^{(k)} \in E_k)$.
In order to do this, we use the previous model function as a baseline for where the next trial point is likely to lie.
There are several ways to add variance around this model to account for innacuracy, including quadratic, linear, or kriging bounds.
The way we have implemented is using a set of interpolation functions as possible errors.
One such set of error functions $\phi^{\epsilon}_i$ are higher order polynomials.
In figure \ref{smf}, we use two cubic terms in addition to the quadratic models.
We extend the vandermode matrix in \ref{reg} to include terms of the error interpolation functions, however we do not force the the lagrange property on these functions.
Instead, we compute the null space of $V$ to have a basis for all error interpolation functions such that $\phi^{\epsilon}_i(y^j) = 0 \forall i,j$.

By sampling these error functions, we are able to create a distribution of possible trial points.
For example, in the following images, the true objective is black, the trust region is green, the model function is blue, and the sampled model function is red.
The current model function's minimizer is magenta, while the sampled model function's minimimzer is yellow.

\begin{figure}[h]
    \centering
    \includegraphics[width=125px]{images/other_polynomial_2.png}
    \includegraphics[width=125px]{images/other_polynomial_3.png}
    \includegraphics[width=125px]{images/other_polynomial_5.png}
    \caption{Sample model functions}
    \label{smf}
\end{figure}

We repeatedly sample the model function $N$, and compute its minimizer.
Collecting these minimizers $h_i$ produces a heuristic distribution of where the next trial point is likely to lie:

\begin{center}
\includegraphics[width=200px]{images/sampled_minimums.png}
\end{center}

We can then set our heuristic $H$ to be $H(E_k) = (\int_{x\in \outertrk} \frac 1 N \sum_{h_i}r(\|x - h_i\| dx)$,
where $r(0) = 1$ and $x>0 \Rightarrow r'(x) < 0$.

This produces the value plot in figure \ref{hvalueplot}, where the magenta contours represent the value of including these points, the green ellipse is the previous trust region, the black lines are the current constraints, and the blue contours are the previous model function.

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/heuristic_value.png}
    \caption{Heuristic value}
    \label{hvalueplot}
\end{figure}



While sampling other model functions, we sample with mean $0$ to ensure that the current model function is most likely.
However, we set the variance of the sampling distribution based on the previous models error:
$\epsilon_k = |\modelkmone(\iteratek) - \modelk(\iteratek)| = |\modelkmone(\iteratek) - f(\iteratek)|$.
We ensure that with fixed probability $p_0$, the sampled model functions have error less than this error at the models best guess for the next minimum.

Without much tuning, this has already improved the ellipsoid searched over the entire trust region.
This distribution may also be useful for decreasing the trust region radius more quickly.


\subsubsection{Subsets of the Trust Region}
\label{simplex_subset_algorithm}

This is another approach to ensuring \ref{accuracy} of the model functions when using the bumping $\xi$ strategy.
Suppose that at some point we are not able to find a sample point passing \ref{impossibly_poised}.
The following idea is based on the fact that if we cannot find a set of sample points poised over then entire polyhedron, they will still be poised over their own convex hull.

Although not computationally feasible for when $A$ has a large number of rows, we are able to sample simplexes to find one containing both the current iterate and likely to contain a descent direction.

\begin{algorithmic}
\Procedure{Volume}{$S$}
    \State \textbf{return} $\frac 1 {n!} \det{[S_1 - S_0, \ldots, S_n - S_0]}$
\EndProcedure
\Procedure{ConstructTrustRegion}{}
    \State $T_{\text{best}} = \emptyset$
    \State $V_{\text{best}} = 0$
    \ForAll{$S \subset \outertrk \cap X$ a simplex}
        \If{$x^{(k)} \not \in S$}
            \State{\textbf{Continue}}
        \EndIf
        \If{$diam(S) < \Delta_k$}
            \State{\textbf{Continue}}
        \EndIf
        \State $V_{\text{trial}} = $ \Call{Volume}{$S$}
        \If{$V_{\text{trial}} > V_{\text{best}}$}
            \State $T_{\text{best}} = S$
            \State $V_{\text{best}} = V_{\text{trial}}$
        \EndIf{}
    \EndFor
    \State \textbf{return} $T_{\text{best}}$
\EndProcedure
\end{algorithmic}


Finding the maximal volume sub-polyhedron is NP-hard, but heuristics can be used to modify one easily computable simplex containing $\iteratek$.
Note that we could apply the heuristic used to look for a ellipsoids within this subroutine as well.

In figure \ref{simplex_iterations}, the left image shows the current constraints and trust region, while the second shows the search for the best simplex containing the model center.
Yellow lines are trial simplexes, while the green simplex has the maximum volume.

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/simplex_iteration.png}
    \includegraphics[width=200px]{images/simplex_search.png}
    \caption{Simplex iteration}
    \label{simplex_iterations}
\end{figure}

\subsection{Circumscribed Ellipse}

Another search type we can perform is find the minimum volume ellispe containing all vertices of $\domain \cap T_k$.
In practice, this is very similar to the bumping $\xi$ strategy because the same constraints must be included within the trust region subproblm.
However, the method may be more likely to include previously evaluated points, and we conceptually ``trust"  points over a larger volume.
An example iteration can be seen in figure \ref{circumscribed_ellipse}:


\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/circumscribed_ellipse.png}
    \caption{Circumscribed ellipse}
    \label{circumscribed_ellipse}
\end{figure}


\section{Analysis}

We plan to leverage convergence results found in \cite{Conejo:2013:GCT:2620806.2621814} which presents very general global convergence results for derivative free algorithms over convex constraints.
A key step in showing using this proof will be satisfying the efficiency condition \ref{efficiency} and the accuracy condition \ref{accuracy}.
If the search space within the trust region subproblem is limited to the inner trust region, than we may have difficulty satisfying \ref{efficiency}.


Once these conditions are satisfied, the remainder of the proof can be found in a more elegant form within \cite{Conejo:2013:GCT:2620806.2621814}.
I am working my way through a more tedius version within the appendix \ref{proof}.

\color{red}
\subsection{Simplexes}

Convergence for the algorithm presented in \ref{simplex_subset_algorithm} follows from the proof given below.

\label{simplexes_exist}


Let $P$ be the polyhedron $P=\{x | Ax \le b\}$ for some $A \in \mathbb R^{m,n}, b \in \mathbb R^{m}$ where $\dim(P) = n$.
Let $c \in P$ and $0 < \Delta < \frac {diam(P)}{2\sqrt{n}}$.
Let $V = \{v_i, i=1\ldots |V|\}$ be the set of vertices of $B(c, \Delta, \infty) \cap P$.

\begin{lemma}
\label{positive_carried}
Suppose that for some $U \subset V$ with $u_1 = v_1 \in U$ there is a representation of 
$c=\sum_{1\le i \le |U|} \lambda_i u_i$ with $\lambda_i \ge 0$ for $i=1\ldots |U|$, $\lambda_1 > 0$, $\sum_{1\le i\le |U|} \lambda_i = 1$, $|U| > n+1$.

Then there is another $U' \subset U$ with $u_1 = v_1 \in U'$ such that
$c=\sum_{1\le i \le |U'|} \lambda_i' u_i$ with $\lambda'_i \ge 0$ for $i=1\ldots |U'|$, $\lambda_1' > 0$, $\sum_{1\le i\le |U'|} \lambda_i = 1$, $|U'| = n+1$.
\end{lemma}

\begin{proof}
Because $|U|>n+1$, the vectors $u_{i} - u_1, i=2\ldots |U|$ are linearly dependent.
Thus, there exist constants $\mu_i,i=2\ldots |U|$ with
\[
\sum_{1 < i \le |U|}\mu_i(u_i - u_1) = 0.
\]
With $\mu_1 = -\sum_{1<i\le|U|}\mu_i$, this is
\[
\sum_{1 \le i \le |U|}\mu_i(u_i - u_1) = 0
\]
 where at least one $\mu_i \ne 0$ and hence at least one $\mu_i > 0$ as well as at least one $\mu_i < 0$.

This means that 
\begin{align}
c &= \sum_{1\le i \le k }(\lambda_i - \alpha_1\mu_i)u_i \label{pos_res} \\
  &= \sum_{1\le i \le k }(\lambda_i + \alpha_2\mu_i)u_i \label{neg_res}
\end{align}
where 
\[\alpha_1 = \min\{ \frac{\lambda_i}{\mu_i}|\mu_i > 0\} = \frac{\lambda_p}{\mu_p}\] and 
\[\alpha_2 = \min\{-\frac{\lambda_i}{\mu_i}|\mu_i < 0\}= \frac{\lambda_q}{\mu_q}.\]
The first representation \ref{pos_res} has $\lambda_p - \alpha_1\mu_p = \lambda_p - \frac{\lambda_p}{\mu_p}\mu_p = 0$,
while the second \ref{neg_res} has
$\lambda_q + \alpha_2\mu_q = \lambda_q - \frac{\lambda_q}{\mu_q}\mu_q = 0$.
Also, 
$\lambda_i + \alpha_2\mu_i \ge 0$ for all $i\ne p$ and $\lambda_i - \alpha_1\mu_i \ge 0$ for all $i\ne q$.

It is not the case that both $p=1=q$ because $\mu_q < 0$ while $\mu_p > 0$.
If $p=1$, then choose the second representation \ref{neg_res}, otherwise choose the first representation \ref{pos_res}.
This ensures that that $v_1$'s coefficient is strictly greater than zero, as we have added a nonnegative term to $\lambda_1$ (either $-\alpha_1$ or $\alpha_2$).

Thus, we have written $c$ in terms of $|U|-1$ vertices, including $v_1$ with a positive coefficient.
Continue this process until $|U| = n+1$.

\end{proof}

\begin{lemma}
\label{all_positive}
Suppose that $c \in interior(P)$.
Then there exists a representation $c = \sum_{i=1}^{|V|} \beta_i v_i$, $\sum_{i=1}^{|V|} \beta_i =1$,
$\beta_i > 0 \quad \forall 1\le i \le |V|$
\end{lemma}

\begin{proof}
Note, $c$ is also in the interior or $B_{\infty}(c, \Delta)$.
There exists an $0 < \epsilon < \Delta$ such that the point $q = c + \epsilon(c - \frac{1}{|V|}\sum_{v \in V}v) \in B_{\infty}(c, \Delta) \cap P$.
Let $q = \sum_{1\le i \le |V|}\alpha_i v_i$ where $\alpha_i \ge 0$ and $\sum_{1\le i \le |V|}\alpha_i = 1$.
Then 
\[
q =  c + \epsilon c - \frac{\epsilon}{|V|}\sum_{v \in V}v
\]
\[
q + \frac{\epsilon}{|V|}\sum_{v \in V}v  = c + \epsilon c
\]
\[
\sum_{1\le i \le |V|}\alpha_i v_i + \frac{\epsilon}{|V|}\sum_{v \in V}v  = (1+\epsilon)c
\]
\[
c = \frac {1}{1+\epsilon} \sum_{1\le i \le |V|}(\alpha_i + \frac{\epsilon}{|V|})v_i
\]

We have $\sum_{1\le i \le |V|}\frac{1}{1+\epsilon} (\alpha_i  + \frac{\epsilon}{|V|}) = \frac{1}{1+\epsilon}(1 + \frac{\epsilon|V|}{|V|})=1$
while $\beta_i = \frac {1}{1+\epsilon}(\alpha_i + \frac{\epsilon}{|V|}) > 0\quad \forall 1\le i \le |V|$.

\end{proof}

\begin{lemma}
\label{boundary_point}
There exists a $u \in V$ such that $\Delta \le \|u - c\|$.
\end{lemma}

\begin{proof}
Let $x_1, x_2 \in P$ be such that $\|x_1 - x_2\| = diam(P)$.
It cannot be that both $x_1,x_2 \in B_{\infty}(c, \Delta)$, as this would imply 
\[
\Delta
< \frac 1 {2\sqrt{n}} diam(P) 
= \frac 1 {2\sqrt{n}} \|x_1 - x_2\| 
\le \frac 1 {2\sqrt{n}} [\|c - x_1\| + \|c - x_2\|] 
\le \frac 1 2 [\|c - x_1\|_{\infty} + \|c - x_2\|_{\infty}] 
\le \frac 1 {2} [\Delta + \Delta] 
= \Delta.
\]

This means that $B_{\infty}(c, \Delta) \cap P \subsetneq P$, so that there exists a $u \in V$ on a constraint of the form $c - \Delta e_i \le u \le c + \Delta e_i$
and hence $\Delta \le \|u - c\| \le \sqrt{n} \Delta$.
\end{proof}

\begin{lemma}
\label{there_is_a_big_simplex}
There exists a simplex $S \subseteq B_{\infty}(c, \Delta) \cap P$ such that $c \in S$ and $diam(S) \ge \Delta$.
\end{lemma}
\begin{proof}
Without loss of generality, let $v_1 = u$ as in \ref{boundary_point}.
We have two cases based on whether there exists an $1\le i \le m$ such that $A_i c = b_i$.

\begin{case}
Suppose that there is such an $i$.
Then consider the set $F = P \cap \{A_i x = b_i\} \cap B_{\infty}(c, \Delta)$.
$F$ is convex as it it the intersection of convex sets, and $dim(F) \le n - 1$.
By Caratheodory's theorem, there exists a set of vertices $U \subseteq V$ with $|U| = n$ such that $c \in conv(U) \subseteq conv(U \cup \{v_1\})$.
Note that $diam(U\cup \{v_1\}) \ge \|x-v_1\| = \Delta$, and $|U\cup \{v_1\}| \le n+1$.
If $|U\cup \{v_1\}| < n+1$, then we can always add more vertices to make $U$ a simplex.
\end{case}
\begin{case}
Suppose that $A_i c < b_i$ for all $i=1\ldots m$.
Then $c$ satisfies the requirements for \ref{all_positive}, and we know that there is a representation 
$c = \sum_{i=1}^{|V|} \beta_i v_i$, $\sum_{i=1}^{|V|} \beta_i =1$,
$\beta_i > 0 \quad \forall 1\le i \le |V|$.
In particular $v_1$ has a positive coefficient, so that by \ref{positive_carried} there exists exists another representation using only vertices of $U\subset V$ where $|U| = n+1$ and $v_1 \in U$.
Once again, if other coefficents are $0$, we can simply add points to ensure $U$ is a simplex.
We once again have $c \in conv(U)$, $diam(U) \ge \|x-v_1\| \ge \Delta$, $|U| = n+1$
\end{case}
\end{proof}

\begin{lemma}
While using quadratic or linear model functions, the search simplex algorithm ensures \ref{accuracy} for all $k$.
\end{lemma}
\begin{proof}


We know that $\Delta_i \le \Delta_max = \frac {diam(\domain)}{2\sqrt{n}} \quad \forall i$.
Thus from \ref{there_is_a_big_simplex}, there is a simplex $S \subset \domain \cal \outertrk $ containing $\iteratek$ with diameter $\Delta_k$.
Only a finite number of simplexes are considered, so the algorithm will find one such $S$.
%In general we need to keep the trust region radius small enough that it does not contain the entire the feasible region.

While using quadratic models, if we let the vertices of $S$ be $V$, and let $v_{i,j} = \frac 1 2 (v_i + v_j) \quad \forall v_i, v_j \in V$, then the set
$\{\iteratek\} \cup V \cup \{v_{i,j} \quad \forall 1\le i, j \le |V|\}$ is posied for interpolation \cite{Ciarlet1971}, \cite{Ciarlet1972}.
Furthermore, we know that $\|\nabla f(x) - \nabla \modelk(x)\| < \kappa_{g} diam(S)$ for some constant $\kappa_{g}$ independent of $k$ \cite{DUMMY:intro_book} (page 29, Theorem 2.13).
We know that linear functionals are unisolvent, so that using the vertices of the simplex will suffice if we use linear model functions.

Because $x^{(k)} \in S$, we have satisfied \ref{accuracy}.

\end{proof}







\subsection{$T_k \cap \domain$: WIP}
\begin{lemma}
Using the bump $\xi$ algorithm, the model is garaunteed to satisfy \ref{efficiency} and \ref{accuracy}.
\end{lemma}

\begin{proof}

THE ATTEMPT:

Let the $V$ be the vertices of $\domain$, $\epsilon = \frac 1 2 \min_{\|u - v\|} \forall u,v \in V$
Consider $B_{\infty}(v, \epsilon) \cap \domain \forall v \in V$.
Take minimum ratio of feasible volume to the balls volume.
\color{red}All trust regions intersect $\domain$ will have this high a ratio.\color{black}
Choose $\xi_{min}$ small enough that it is possible to find lambda poised points within this problem.
Then we can drop the check for \ref{impossibly_poised}

By increasing the radius of the $L_{\infty}$ ball, we decrease the volume on the inside (letting more constraints intersect the trust region makes the feasible region smaller).
Maybe I should have considered large balls, with a maximum ball being the same as in the proof for simplexes?

The case of linear model functions might be easier: they are always poised when taken to be the vertices of a non-degenerate simplex.

\end{proof}

\subsection{Ellipsoid Searches: WIP}

If we have that the points are poised for regression: \cite{DUMMY:intro_book}, page 29, Theorem 2.13
Theorem 12.2.2 of \cite{Conn:2000:TM:357813} on page 457 shows that such a point exists (the generalized Cauchy point).
This point exists within $\domain \cap TR$.
Any point can be written as a convex combination of $n$ vertices.
With quadratic models, we have a choice of up to $\frac{(n+1)(n+2)}{2}$ vertices to choose from.
We could find the $n+1$ points required for the cauchy point (?) and the model center.
$\frac{(n+1)(n+2)}{2} \ge 2(n+1)\forall n\ge 2 \in \mathbb N$

THE CAUCHY POINT DEPENDS ON WHICH VERTICES WE CHOOSE.
If we are smart about how to choose the ellipse, we may be able to force it to include the cauchy point.
THE CAUCHY POINT DEPENDS ON WHICH ELLIPSE WE CHOOSE.
could be iterative
We get \ref{accuracy} for free for the inner trust region.
\color{black}





\input{ending}

\color{red}
\subsection{Test Problems}

I just learned about the Hock-Schittkowski collection.

They include equality constraints.

\begin{verbatim}
https://github.com/uqfoundation/mystic/blob/master/models/schittkowski.py
\end{verbatim}

CITE

Also based on Cute.

This seems SO MUCH more natural than trying to modify the least squares problems to introduce constraints.

I could run the polyhedral and spherical algorithms relatively easily. (ellipsoidal algorithm has constraints that are for 2d only.)

Maybe modify them to only have two partially-quantifiable constraints?



\newpage

\subsection{Interpretation}

The spherical none didn't actually converge.

The feasible region intersect the trust region is clearly the best, however this may have the most difficult convergence proof.

Line segments do well.

Linear does better than quadratic.


\section{Extensions to more general derivative free convex constraints}


The ellipse may be preferrable when we wish to avoid getting too close to the constraints.

We first compute an interpolation set poised for regressing a set of model functions, which we choose to be quadratic functions.
Although we have enough sample points to construct a quadratic model of the constraints, we only construct linear models to avoid the complexity of Quadratically Constrained Quadratic Programming which is NP-hard.

Should $\rho$ include the constraints?



\newpage

\appendix

\input{table_of_notation}

\newpage

\bibliography{bibliography}
\bibliographystyle{ieeetr}

\end{document}


