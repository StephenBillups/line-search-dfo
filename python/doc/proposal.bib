@BOOK{DUMMY:1,
AUTHOR="John Doe",
TITLE="The Book without Title",
PUBLISHER="Dummy Publisher",
YEAR="2100",
}
@article {DUMMY:Biegler,
author = {Eason, John P. and Biegler, Lorenz T.},
title = {A trust region filter method for glass box/black box optimization},
journal = {AIChE Journal},
volume = {62},
number = {9},
issn = {1547-5905},
url = {http://dx.doi.org/10.1002/aic.15325},
doi = {10.1002/aic.15325},
pages = {3124--3136},
keywords = {mathematical modeling, multiscale modeling, optimization, nonlinear programming, trust region methods},
year = {2016},
}
@TECHREPORT{DUMMY:Fletcher,
    author = {Roger Fletcher and Sven Leyffer and Roger Fletcher and Sven Leyffer},
    title = {A brief history of filter methods},
    institution = {},
    year = {2006}
}
@article{DUMMY:Brekelman,
title = "Constrained optimization involving expensive function evaluations: A sequential approach ",
journal = "European Journal of Operational Research ",
volume = "160",
number = "1",
pages = "121 - 138",
year = "2005",
note = "Applications of Mathematical Programming Models ",
issn = "0377-2217",
doi = "http://dx.doi.org/10.1016/j.ejor.2003.10.009",
url = "http://www.sciencedirect.com/science/article/pii/S0377221703007094",
author = "Ruud Brekelmans and Lonneke Driessen and Herbert Hamers and Dick den Hertog",
keywords = "Nonlinear programming",
keywords = "Derivative free optimization",
keywords = "Trust region",
keywords = "Filter ",
abstract = "This paper presents a new sequential method for constrained nonlinear optimization problems. The principal characteristics of these problems are very time consuming function evaluations and the absence of derivative information. Such problems are common in design optimization, where time consuming function evaluations are carried out by simulation tools (e.g., FEM, CFD). Classical optimization methods, based on derivatives, are not applicable because often derivative information is not available and is too expensive to approximate through finite differencing. The algorithm first creates an experimental design. In the design points the underlying functions are evaluated. Local linear approximations of the real model are obtained with help of weighted regression techniques. The approximating model is then optimized within a trust region to find the best feasible objective improving point. This trust region moves along the most promising direction, which is determined on the basis of the evaluated objective values and constraint violations combined in a filter criterion. If the geometry of the points that determine the local approximations becomes bad, i.e. the points are located in such a way that they result in a bad approximation of the actual model, then we evaluate a geometry improving instead of an objective improving point. In each iteration a new local linear approximation is built, and either a new point is evaluated (objective or geometry improving) or the trust region is decreased. Convergence of the algorithm is guided by the size of this trust region. The focus of the approach is on getting good solutions with a limited number of function evaluations. "
}

@inproceedings{DUMMY:CombineTrustAndLine,
 author         = {J. Nocedal and Y. Yuan},
 title          = {Combining trust region and line search techniques},
 abstract       = {We propose an algorithm for nonlinear optimization that
                   employs both trust region techniques and line searches.
                   Unlike traditional trust region methods, our algorithm does
                   not resolve the subproblem if the trial step results in an
                   increase in the objective function, but instead performs a
                   backtracking line search from the failed point.
                   Backtracking can be done along a straight line or along a
                   curved path. We show that the new algorithm preserves the
                   strong convergence properties of trust region methods.
                   Numerical results are also presented.},
 summary        = {An algorithm for nonlinear optimization that employs both
                   trust-region techniques and linesearches is proposed. This
                   algorithm does not resolve the subproblem if the trial step
                   results in an increase in the objective function, but
                   instead performs a backtracking linesearch from the failed
                   point. Backtracking can be done along a straight line or
                   along a curved path. It is shown that the algorithm
                   preserves the strong convergence properties of trust-region
                   methods. Numerical results are presented.}}
@article{DUMMY:linesearch_global,
  author    = {Andreas W{\"{a}}chter and
               Lorenz T. Biegler},
  title     = {Line Search Filter Methods for Nonlinear Programming: Motivation and
               Global Convergence},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {16},
  number    = {1},
  pages     = {1--31},
  year      = {2005},
  url       = {http://dx.doi.org/10.1137/S1052623403426556},
  doi       = {10.1137/S1052623403426556},
  timestamp = {Fri, 25 Jun 2010 01:00:00 +0200},
  biburl    = {http://dblp2.uni-trier.de/rec/bib/journals/siamjo/WachterB05},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{DUMMY:linesearch_local,
  author    = {Andreas W{\"{a}}chter and
               Lorenz T. Biegler},
  title     = {Line Search Filter Methods for Nonlinear Programming: Local Convergence},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {16},
  number    = {1},
  pages     = {32--48},
  year      = {2005},
  url       = {http://dx.doi.org/10.1137/S1052623403426544},
  doi       = {10.1137/S1052623403426544},
  timestamp = {Fri, 25 Jun 2010 01:00:00 +0200},
  biburl    = {http://dblp2.uni-trier.de/rec/bib/journals/siamjo/WachterB05a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@book{DUMMY:intro_book,
 author = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
 title = {Introduction to Derivative-Free Optimization},
 year = {2009},
 isbn = {0898716683, 9780898716689},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
} 
@article{DUMMY:trust_funnel_dfo,
  author    = {R. Sampaio and L. Toint},
  title     = {A TRUST-FUNNEL METHOD FOR NONLINEAR OPTIMIZATION PROBLEMS WITH GENERAL NONLINEAR CONSTRAINTS AND ITS APPLICATION TO DERIVATIVE-FREE OPTIMIZATION, most information in this cite is wrong},
  journal   = {{NAXYS} Namur Center for Complex Systems},
  volume    = {61},
  number    = {5000},
  pages     = {1--31},
  year      = {2015},
  url       = {http://www.unamur.be/sciences/naxys},
  doi       = {10.1137/S1052623403426556},
  timestamp = {Mon, 26 Jan 2015 01:00:00 +0200},
  biburl    = {http://dblp2.uni-trier.de/rec/bib/journals/siamjo/WachterB05},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@Article{DUMMY:original_filter,
author="Gould, N. I. M.
and Toint, Ph. L.",
title="Nonlinear programming without a penalty function or a filter",
journal="Mathematical Programming",
year="2010",
volume="122",
number="1",
pages="155--196",
abstract="A new method is introduced for solving equality constrained nonlinear optimization problems. This method does not use a penalty function, nor a filter, and yet can be proved to be globally convergent to first-order stationary points. It uses different trust-regions to cope with the nonlinearities of the objective function and the constraints, and allows inexact SQP steps that do not lie exactly in the nullspace of the local Jacobian. Preliminary numerical experiments on CUTEr problems indicate that the method performs well.",
issn="1436-4646",
doi="10.1007/s10107-008-0244-7",
url="http://dx.doi.org/10.1007/s10107-008-0244-7"
}
@article{DUMMY:sqp_filter,
title = "Sequential quadratic programming for large-scale nonlinear optimization ",
journal = "Journal of Computational and Applied Mathematics ",
volume = "124",
number = "1â€“2",
pages = "123 - 137",
year = "2000",
note = "Numerical Analysis 2000. Vol. IV: Optimization and Nonlinear Equations ",
issn = "0377-0427",
doi = "http://dx.doi.org/10.1016/S0377-0427(00)00429-5",
url = "http://www.sciencedirect.com/science/article/pii/S0377042700004295",
author = "Paul T. Boggs and Jon W. Tolle",
keywords = "Sequential quadratic programming",
keywords = "Nonlinear optimization",
keywords = "Newton methods",
keywords = "Interior-point methods",
keywords = "Local",
keywords = "Trust-region methods convergence",
keywords = "Global convergence ",
abstract = "The sequential quadratic programming (SQP) algorithm has been one of the most successful general methods for solving nonlinear constrained optimization problems. We provide an introduction to the general method and show its relationship to recent developments in interior-point approaches, emphasizing large-scale aspects. "
}


@TECHREPORT{DUMMY:Fletcher_abrief,
    author = {Roger Fletcher and Sven Leyffer and Roger Fletcher and Sven Leyffer},
    title = {A brief history of filter methods},
    institution = {},
    year = {}
}

@Article{DUMMY:Colson2004,
author="Colson, Beno{\'i}t",
title="Trust-region algorithms for derivative-free optimization and nonlinear bilevel programming",
journal="Quarterly Journal of the Belgian, French and Italian Operations Research Societies",
year="2004",
volume="2",
number="1",
pages="85--88",
abstract="We briefly describe the contents of the author's PhD thesis (see Colson 2003) discussed on July 2003 at the University of Namur (Belgium) and supervised by Philippe L. Toint. The contributions presented in this thesis are the development of trust-region methods for solving two particular classes of mathematical programs, namely derivative-free optimization (DFO) problems and nonlinear bilevel programming problems. The thesis is written in English and is available via the author.",
issn="1619-4500",
doi="10.1007/s10288-003-0020-8",
url="http://dx.doi.org/10.1007/s10288-003-0020-8"
}



@article{DUMMY:SQPFilter,
title = "Global convergence of a trust-region SQP-filter algorithm for general nonlinear programming",
abstract = "A trust-region SQP-filter algorithm of the type introduced by Fletcher and Leyffer [Math. Program., 91 (2002), pp. 239-269] that decomposes the step into its normal and tangential components allows for an approximate solution of the quadratic subproblem and incorporates the safeguarding tests described in Fletcher, Leyffer, and Toint [On the Global Convergence of an SLP-Filter Algorithm, Technical Report 98/13, Department of Mathematics, University of Namur, Namur, Belgium, 1998; On the Global Convergence of a Filter-SQP Algorithm, Technical Report 00/15, Department of Mathematics, University of Namur, Namur, Belgium, 2000] is considered. It is proved that, under reasonable conditions and for every possible choice of the starting point, the sequence of iterates has at least one first-order critical accumulation point.",
keywords = "Convergence theory, Filter methods, Nonlinear optimization, Sequential quadratic programming",
author = "Roger Fletcher and Gould, {Nicholas I M} and Sven Leyffer and Toint, {Philippe L.} and Andreas WÃ¤chter",
year = "2003",
doi = "10.1137/S1052623499357258",
volume = "13",
pages = "635--659",
journal = "SIAM Journal on Optimization",
issn = "1052-6234",
publisher = "Society for Industrial and Applied Mathematics Publications",
number = "3",
}

@Article{DUMMY:Rios2013,
author="Rios, Luis Miguel
and Sahinidis, Nikolaos V.",
title="Derivative-free optimization: a review of algorithms and comparison of software implementations",
journal="Journal of Global Optimization",
year="2013",
volume="56",
number="3",
pages="1247--1293",
abstract="This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.",
issn="1573-2916",
doi="10.1007/s10898-012-9951-y",
url="http://dx.doi.org/10.1007/s10898-012-9951-y"
}

@Inbook{DUMMY:PowellRadialBasis,
author="Powell, M. J. D.",
editor="M{\"u}ller, Manfred W.
and Buhmann, Martin D.
and Mache, Detlef H.
and Felten, Michael",
title="Recent research at Cambridge on radial basis functions",
bookTitle="New Developments in Approximation Theory: 2nd International Dortmund Meeting (IDoMAT) '98, Germany, February 23--27, 1998",
year="1999",
publisher="Birkh{\"a}user Basel",
address="Basel",
pages="215--232",
isbn="978-3-0348-8696-3",
doi="10.1007/978-3-0348-8696-3_14",
url="http://dx.doi.org/10.1007/978-3-0348-8696-3_14"
}



@report{DUMMY:leastsquares,
author = {Wild, S M},
number = {ANL/MCS-P5120-0414},
title = {Solving Derivative-Free Nonlinear Least Squares with {POUNDERS}},
month = {April},
year = {2014 (Revised June 2015)},
type = {Preprint},
institution = {Argonne National Laboratory}
}


@Article{DUMMY:review,
author="Rios, Luis Miguel
and Sahinidis, Nikolaos V.",
title="Derivative-free optimization: a review of algorithms and comparison of software implementations",
journal="Journal of Global Optimization",
year="2013",
volume="56",
number="3",
pages="1247--1293",
abstract="This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.",
issn="1573-2916",
doi="10.1007/s10898-012-9951-y",
url="http://dx.doi.org/10.1007/s10898-012-9951-y"
}

@article{DUMMY:typesofconstraints,
	title = {A Taxonomy of Constraints in Simulation-Based Optimization},
	year = {2015},
	abstract = {<p>The types of constraints encountered in black-box and simulation-based optimization problems differ significantly from those treated in nonlinear programming. We introduce a character- ization of constraints to address this situation. We provide formal definitions for several constraint classes and present illustrative examples in the context of the resulting taxonomy. This taxonomy, denoted QRAK, is useful for modeling and problem formulation, as well as optimization software development and deployment. It can also be used as the basis for a dialog with practitioners in moving problems to increasingly solvable branches of optimization.<br />
	<br />
	\&nbsp;</p>
},
	author = {S. Le Digabel and S. M. Wild}
}






@article{DUMMY:FasanoLLR14,
  author    = {Giovanni Fasano and
               Giampaolo Liuzzi and
               Stefano Lucidi and
               Francesco Rinaldi},
  title     = {A Linesearch-Based Derivative-Free Approach for Nonsmooth Constrained
               Optimization},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {24},
  number    = {3},
  pages     = {959--992},
  year      = {2014},
  url       = {https://doi.org/10.1137/130940037},
  doi       = {10.1137/130940037},
  timestamp = {Thu, 08 Jun 2017 01:00:00 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/siamjo/FasanoLLR14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DUMMY:filterpattern,
  author    = {M. A. Abramson and 
				C. Audet and 
				G. Couture and
				J. E. Dennis Jr.,},
  title     = {Filter pattern search algorithms for mixed variable constrained optimization problems},
  journal   = {Pacific Journal of Optimization},
  volume    = {17},
  pages     = {477--500},
  year      = {2007}
}








@article{DUMMY:activeset1,
author = { Serge   Gratton  and  Philippe   L.   Toint  and  Anke   TrÃ¶ltzsch },
title = {An active-set trust-region method for derivative-free nonlinear bound-constrained optimization},
journal = {Optimization Methods and Software},
volume = {26},
number = {4-5},
pages = {873-894},
year = {2011},
doi = {10.1080/10556788.2010.549231},

URL = { 
        http://dx.doi.org/10.1080/10556788.2010.549231
    
},
eprint = { 
        http://dx.doi.org/10.1080/10556788.2010.549231
    
}

}


@article{DUMMY:LiuzziLS10,
  added-at = {2010-12-06T00:00:00.000+0100},
  author = {Liuzzi, Giampaolo and Lucidi, Stefano and Sciandrone, Marco},
  biburl = {http://www.bibsonomy.org/bibtex/2b8da39c824411b7b8da53228fa2a6894/dblp},
  ee = {http://dx.doi.org/10.1137/090750639},
  interhash = {9c9d7e996fdbd11a7bce2b364a5e32f5},
  intrahash = {b8da39c824411b7b8da53228fa2a6894},
  journal = {SIAM Journal on Optimization},
  keywords = {dblp},
  number = 5,
  pages = {2614-2635},
  timestamp = {2010-12-07T11:33:56.000+0100},
  title = {Sequential Penalty Derivative-Free Methods for Nonlinear Constrained Optimization.},
  url = {http://dblp.uni-trier.de/db/journals/siamjo/siamjo20.html#LiuzziLS10},
  volume = 20,
  year = 2010
}



@misc{DUMMY:review2,
author = {A.L. Custodio and K. Scheinberg and L.N. Vicente},
title = {Methodologies and Software for Derivative-free Optimization},
year = {2017},
URL = { https://www.mat.uc.pt/~lnv/papers/dfo-survey.pdf }
}

@misc{DUMMY:augmented,
author = {Charles Audet and Sebastien Le Digabel and Mathilde Peyrega},
title = {A derivative-free trust-region augmented Lagrangian algorithm},
year = {2016},
URL = { http://www.optimization-online.org/DB_HTML/2016/07/5530.html }
}

