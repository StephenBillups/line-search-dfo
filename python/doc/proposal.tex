\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{varwidth}% http://ctan.org/pkg/varwidth
\usepackage{xspace}
\usepackage{placeins}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsfonts}

%\theoremstyle{definition}
%\newtheorem*{dfn}{A Reasonable Definition}


\DeclareMathOperator*{\argmin}{arg\,min}


\title{Derivative Free Model-Based Methods for Local Constrained Optimization}
\author{Trever Hallock}
%\institute{CU Denver}
%\date{January 6, 2012}
% Remove the % from the previous line and change the date if you want a particular date to be displayed; otherwise, today's date is displayed by default.

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother


\begin{document}


\algnewcommand{\algorithmicgoto}{\textbf{go to}}%
\algnewcommand{\Goto}{\algorithmicgoto\xspace}%
\algnewcommand{\Label}{\State\unskip}


\maketitle


\tableofcontents

\section{Introduction}

This paper will discuss research topics for derivative free optimization.
It begins with an introduction to the context and goals of derivative free optimization (DFO) supplemented by some of the recent advancement made in the field.
It then details several future research directions and one in particular (filter methods) that has been studied.
The focus is on local search algorithms for constrained derivative free optimization.


\section{Derivative-free Mentality}

Derivative free optimization (DFO) is motivated by programs in which derivative information is unknown, deceptive or otherwise impractical to compute.
For example, this can arise when the objective is the result of a simulation that does not admit automatic differentiation.
The general strategy is to converge to a first or second order critical point while evaluating the function as few times as possible.

It seems like several papers have mentioned
\begin{itemize}
\item copyright issues in code
\item user laziness
\item large tolerance (lack of regard to asymptotic convergence)
\item parameter tuning
\end{itemize}

\subsubsection{Derivatives are convenient}

The lack of derivative information means that DFO methods are at a disadvantage when compared to their counterparts in nonlinear optimization.
First and second derivative information is explicit in algorithms with quadratic convergence such as Newton's method.
They are also present in conditions for convergence results such as Wolf's, Armijo or Goldstien for line search methods.
Additionally, stopping criteria usually involve a criticality test involving derivatives.

\subsubsection{Problem statement}

In more detail, DFO methods consider nonlinear, constrained optimization problems of the form

\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}} & & f(x, S(x)) \\
& \text{subject to} & & g(x, S(x)) \leq 0, \; i = 1, \ldots, m_{\mathcal{I}} \\
& & & h_i(x, S(x)) = 0, \; i = 1, \ldots, m_{\mathcal{E}}
\end{aligned}
\end{equation*}

which give rise to different classes of derivative-free optimization based on properties of
$S : \mathbb{R}^n \to \mathbb{R}^m$,
$f : \mathbb{R}^{n+m} \to \mathbb{R}$,
$g : \mathbb{R}^{n+m} \to \mathbb{R}^{m_{\mathcal{I}}}$, and
$h : \mathbb{R}^{n+m} \to \mathbb{R}^{m_{\mathcal{E}}}$.

Here, we assume that $S(x)$ is a black-box function, meaning that we have no information about its derivatives.
We will introduce several different forms this problem can take before narrowing in on our interest.

One branch of DFO is concerned with noisy evaluations of $S$.
Noisy functions can be categorized as either deterministic or random.
Deterministic means that the genuine objective is not always evaluated accurately, but measurement error (?) will not change across multiple function calls at the same point.
Random means that each point in the domain is associated with a distribution of possible values the objective may return.
In this paper we assume $S$ is not noisy.

If $g(x, S(x)) = g(x)$ and $h(x, S(x)) = h(x)$, then the derivatives of $g$ and $h$ are known, so the objective is the only derivative free function.
We are primarily interested in the case where no derivative information of $g$ and $h$ are known, for example if they are also output from a simulation used to evaluate the objective.
This means that each call to the objective gives values of the constraints as well, and vica versa.
(Encouraging higher order models of the constraints?)
This produces $g(x, S(x))=g(S(x))$ and $h(x,S(x))=h(S(x))$.

One common such case is to include bound constraints of the form $b_{L} \le x \le b_{U}$ for some $b_{L} < b_{U}$, which gives rise to Box Constrained DFO (BCDFO).
This is one of the better studied cases, with several software packages available: CITE.

However, although derivatives of $S$ are not known, we do assume that $S$, $f$, $g$, and $h$ are all continuously twice differentiable.

If the constraints can be evaluated at points outside the feasible region, the constraints are called relaxable constraints.
Some problems additionally contain ``hidden" constraints which are not explicit in the model but merely result in a notification that the objective could not be evaluated at the requested point.
This may mean that it is not possible to tell how close to a ``hidden" constraint the current iterate lies.

Again, we only consider only local search algorithms for this problem that seek a first or second order stationary point.
One interesting topic within the research is different shapes of the feasible regions near critical points.
Although constrained DFO has attracted much interest in terms of papers, there are still few libraries for constrained derivative free optimization.
An area that may not have seen much progress is that of the NLP responses to constraints that leave narrow feasible regions near a critical point.
This is of particular interest to DFO because reducing the dimension near a critical point could significantly harm the geometry of the poised set.
One approach may be to redefine the poisedness as the maximum value of the model function over only the feasible region intersected with the trust region.

Another area that received attention recently is that of imposing structure on $f$, $g$, and $h$.
For example, when $f$ takes the form of a least squares error, some improvements can be made. CITE Stephen Wild

%Many DFO methods simply let $f(x,S(x)) = S(x)$.

An intriguing form of the previous program is presented by \cite{DUMMY:Biegler} and given here:
\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}} & & f(x, y, z) \\
& \text{subject to} & & g(x, y, z) \leq 0 \\
& & & h(x, y, z) = 0 \\
& & & y = S(x) \;  \\
\end{aligned}
\end{equation*}
where
$S : \mathbb{R}^n \to \mathbb{R}^m$,
$f : \mathbb{R}^{n+m+k} \to \mathbb{R}$,
$g : \mathbb{R}^{n+m+k} \to \mathbb{R}^{m_{\mathcal{I}}}$, and
$h : \mathbb{R}^{n+m+k} \to \mathbb{R}^{m_{\mathcal{E}}}$.

Biegler assumes that the dimension of $x$ is small compared to the dimension of $y$ and $z$.
This permits the algorithmic approach of relaxing $y=S(x)$.

\subsection{Approaches for Local Search Algorithms}

In this section we discuss several of the approaches taken when derivative information is not known.

\subsubsection{Automatic Differentiation}

When $S$ is the result of a simulation for which the source code is available, one convenient approach is to perform automatic differentiation.
Although derivatives of complicated expressions resulting from code structure are difficult to work with on paper, the rules of differentiation can be applied algorithmically to calculate derivatives.
However, the nature of the code or problem can make this very difficult: for example with combinatorial problems that rely heavily on if statements.
(?)

\subsubsection{Finite difference methods}
Finite difference methods can be used to approximate the derivative of a function $f$.
A common approximation is given by $\nabla f(x) \approx (\frac{f(x+he_i) - f(x-he_i)}{2h})_i$ for some small $h$.
This may work well but can have issues with unlucky iterates.
The number of function evaluations tends to grow large with the dimension and number of iterations the algorithm performs as information is only gathered near the current iterate when $h$ is small (which is required for accurate derivatives).
Because of the large number of function evaluations required for finite difference schemes, these can be unusable.

CITE

\subsubsection{Direct search methods}

Another approach is to use direct search methods that do not explicitly estimate the derivative but evaluate the objective on a pattern or other structure to find a descent direction.
Examples of this include Coordinate descent and other pattern based search methods.
One of the most popular direct search method is Nelder Mead (it is implemented in fminsearch in matlab) although it is proven to not converge in (synonym: bad) cases.
These methods can be robust but ignore information because they do not use derivatives.


(0th derivative)

\subsubsection{Model based methods}
In this paper, we are concerned with model based methods that minimize a model used to approximation of the unknown objective (by regression or kriging or ...).
The algorithm can then use derivative information from the model to find a descent direction.
These methods require an adequate geometry of sampled points to ensure convergence.

More specifically, regression based methods require that set the function is evaluated at must be $\Lambda$ poised for a fixed constant $\Lambda$.
This ensures that the Vandermode matrix used to find the coefficients used to express the model function in terms of a basis of Lagrange polynomials is well conditioned.
$\dots$.




\subsection{The Derivative Free Trust Region Method}

The overall description of the trust region framework is that a set of poised points are chosen for some radius $\Delta$ about the current iterate.
The objective/constraint are then evaluated at these point to construct a model function as a linear combination of some set of basis functions.
Next, the model is minimized over this trust region to and the minimum becomes the trial point.
The objective is evaluated at the trail point and a measure of reduction $\rho$ is computed.
If $\rho$ implies that sufficient reduction has been made, the trial point is accepted as the new iterate.
Otherwise, the trust region is reduced.

\subsubsection{Algorithmic Framework}

For unconstrained optimization, the framework can be described with these steps:

\begin{enumerate}
	\item Define $m_k(x) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x-x^{(k)}) + \frac 1 2 (x-x^{(k)})^T\nabla^2f(x^{(k)})(x-x^{(k)})$
	\begin{itemize}
		\item $\nabla f(x^{(k)})$ and $\nabla^2 f(x^{(k)})$ must be approximated
		\item There are geometric properties of the sample set that must be satisfied
	\end{itemize}
	\item If $\nabla m_k(x) < t$ stop
	\item Solve the Trust region subproblem: $s^{(k)} = \argmin_{s\in B_{x^{(k)}}, \Delta_k)} m_k(x^{(k)} + s)$
	\item Test for improvement
	\begin{itemize}
		\item $\rho_k = \frac{f(x^{(k)}) - f(x^{(k)}+s^{(k)})}{m_k(x^{(k)}) - m_k(x^{(k)}+s^{(k)})}$ measures the actual improvement over predicted improvement
		\item If $\rho$ is small, $x^{(k+1)}=x^{(k)}$ (reject) and decrease radius
		\item If $\rho$ is intermediate, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and decrease radius
		\item If $\rho$ is large, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and increase radius
	\end{itemize}
\end{enumerate}

Our goal is generalize this framework to handle constraints, where we must take care to reduce constraint violation and account for constraint's model's accuracy.
?

The choice of model functions can have some affect on the convergence rate, as Powell showed in CITE.
Model functions are usually chosen to be fully linear or fully quadratic, terms describing how the model's error grows as a function of the trust region radius.
Intuitively, radial basis functions may have some ?conveniences? because the algorithm makes claims about the accuracy of the function over a trust region.


%\begin{equation*}
%\begin{aligned}
%& \underset{x}{\text{minimize}} & & f(x, S(x)) \\
%& \text{subject to} & & g_i(x) \leq 0, \; i = 1, \ldots, m \\
%& & & h_i(x) = 0, \; i = 1, \ldots, n
%\end{aligned}
%\end{equation*}


\section{Literature Review}

The original filter method was proposed by Gould in \cite{DUMMY:original_filter}. The motivation for the filter method was that the algorithm does not need to tune any parameters as in penalty or merit methods.

A recent paper from September 2016 \cite{DUMMY:Biegler} implements a derivative-free trust region filter method for solving the the program given above (REFERENCE).
This is more convenient for some problems than the algorithm we consider as it allows for the objective to depend on other glass box functions of the input and multiple outputs of the black box function.
The authors compare their algorithm to finite difference methods as well as kriging on three different applications from Chemistry. Within the algorithm, the current iterate is always feasible with respect to all inequalities except for the constraint $y=d(z)$.


In the 2006 paper \cite{DUMMY:Fletcher} Fletcher reviews how filter methods have developed.
The only references to derivative-free versions of the filter method are those applied to pattern based (direct) methods.
However, the authors outline trust region filter methods along with several other variants.

In Brekelman's paper \cite{DUMMY:Brekelman}, a trust region filter method is developed to minimize function evaluations by constructing linear model functions.
This paper also uses experimental designs to choose following iterates.


Within \cite{DUMMY:linesearch_global} and \cite{DUMMY:linesearch_local} Biegler uses a filter method to ensure global convergence within a line search framework.
We experimented with this before deciding combining line search with the derivative-free trust region approach was not a natural approach. (There have been attempts to employ both of these frameworks in \cite{DUMMY:CombineTrustAndLine}.)


Within  \cite{DUMMY:intro_book} derivative-free methods are developed in detail.
This contains a good explanation of ensuring geometry of the current set with poisedness for unconstrained problems and also covers other derivative-free methods including direct-search and line search.


Within \cite{DUMMY:trust_funnel_dfo} Toint generalizes the filter method with the notion of a trust funnel.
This is for glass box functions as it does not include any derivative-free methods.


Within \cite{DUMMY:sqp_filter} a sequential quadratic programming method is applied to the filter method?


Colson has also applied filter techniques to derivative-free optimization in his 2004 Ph.D. thesis \cite{Colson2004}.
This focuses on bilevel programming.


I based my algorithm on the trust region filter method described in:

GLOBAL CONVERGENCE OF A TRUST-REGION SQP-FILTER
ALGORITHM FOR GENERAL NONLINEAR PROGRAMMING∗
ROGER FLETCHER†, NICHOLAS I. M. GOULD‡, SVEN LEYFFER†,
PHILIPPE L. TOINT§, AND ANDREAS WACHTER ¨ ¶
I need to find a bibtex reference of this paper to put in the bibliography.



% In order to give this talk as is, I need to be prepared to talk about
% and preferably have optional slides for
% Coordinate descent, Nelder mead (not implicit filtering http://www4.ncsu.edu/~ctk/imfil.pdf)
% Wolf, Armijo, or Goldstein
% trust region subproblem
% know values of \eta_1, \eta_2
% interpolation may involve frebenius norm/enforcing sparsity
% http://www.sciencedirect.com/science/article/pii/S0167819103000139
% Should use the word black box early on

the two additional review papers.



\section{Possible future work}

\subsection{Future Directions}

Here we outline the future research directions we will investigate.

%Possible future research directions fall among the follows topics:

%\begin{itemize}
%\item Convert classical NLP Algorithms to DFO
%\item Consider different DFO Goals
%\item Parallelize DFO Algorithms
%\item Explore different model functions
%\item Impose structure
%\end{itemize}


\subsection{NLP Methods}

Future work includes converting nonlinear algorithms to a derivative free context.
In more detail, we start by choosing a nonlinear optimization algorithm, such as the ones listed below.
The algorithm will contain steps that reference derivatives, for example in either computation of a step direction or in a criticality measure.
To convert the algorithm to a DFO context, the derivative evaluations are replaced with derivatives of a model function over a trust region.
However, once this has been done, the algorithm may require further modifications to ensure convergence.
For example, in the example we give later of converting the filter method, we had to change trust region management rules to ensure convergence.
Not only did the trust region simply not go to zero in the filter method which is required in the DFO context, but we needed to decrease the trust region with a weakened criticality measure.


Although we began with the filter method, recent progress has been done by other researchers with this method.
There are several consideration to make when selecting an NLP method.
One of the primary concerns is how well it is extended to the trust region framework, for example this conflicts with several goals of line search.
Another concern is that filter methods only produce feasible iterates once the algorithm has converged.
This is different than ``any time algorithms" that maintain feasibility so that the algorithm can be stopped at any time to yield feasible guess.
The longer an any time algorithm is run, the better the returned value is, until optimality is reached.

Here we outline several potential nonlinear programming algorithms of interest.
For some of these algorithms, others have made progress in translating to a DFO context.

\begin{itemize}
	\item Line search (CITE)
	\item Filter method
	\begin{itemize}
		\item The first filter methods in DFO were introduced in 2004 CITE within the context of pattern searches
		\item Since then, progress has also been made in a trust region framework CITE, CITE, CITE
	\end{itemize}
	\item Active Set (CITE)
	\item Augmented Lagrangian (CITE)
	\item Penalty
	\begin{itemize}
		\item Abramson \& Audet, 2006
		\item Abramson et al. 2009c
		\item audet et al. 2008b
		\item Audet \& Dennis, 2006
		\item sequential penalty merit functions Liuzzi et al., 2010
		\item smoothed exact $l-\infty$ penalty function Liuzzi \& Lucidi, 2009
		\item exact penalty merit function Fasano, Liuzzi, Lucidi, \& Rinalsdi, 2014; Gratton \& Vicente, 2014
	\end{itemize}
	\item Progressive Barrier
	\begin{itemize}
		\item Audet \& Dennis, 2009
	\end{itemize}
	\item Interior Point (CITE)
\end{itemize}

The general approach is to replace derivatives of $f$, $g_i$, $h_i$ with derivatives of the model function.
Then other aspects of the algorithm may have to be changed in order to ensure convergence.
I have considered line search methods as well as Filter methods which are discussed below.





\subsection{Other directions}
\subsubsection{Goals}
We could consider explicitly modeling the runtime of evaluating the objective function.
We would assume that the runtime varies continuously with $x$.
For example, in some problems from PDE's the runtime of a simulation may involve the ``stiffness" of $\ldots$.
In these situations, it may be advisable to consider the runtime cost of the function when evaluating new points:
choosing points that remain poised with respect to the geometry but minimize evaluation time.

%We could minimize the number of future function evaluations given a stochastic model of what the new function values could be.

% We could consider the following bicriteria optimization problem:
%
% \begin{equation*}
% \begin{aligned}
% & \underset{x}{\text{minimize}} & & (f(x, S(x)), \sum_i r(x_i)) \\
% & \text{subject to} & & g_i(x) \leq 0, \; i = 1, \ldots, m \\
% & & & h_i(x) = 0, \; i = 1, \ldots, n
% \end{aligned}
% \end{equation*}
%
% \begin{itemize}
% \item $x_i$ is the generated points where $f(x)$ are evaluated
% \item $r(x)$ is the runtime of evaluating $f(x)$, or possibly worst case runtime
% \item This may require applications with smooth variability in runtime, so that a models can be constructed or derivatives can be taken.
% \item In order to provide appreciable improvement, we also desire significant variation in $r(x)$.
% \end{itemize}



\subsubsection{Parallelization}

Parallelization is another topic receiving attention.
Modern super computer design can be exploited in several different ways, including parallelization of linear algebra kernels or of function and/or derivative evaluations within the algorithm.
More interestingly is from the modification of the basic algorithms which increase the degree of intrinsic parallelism.
This can come from performing multiple function and/or derivative evaluations.


NOMAD?
SNOBFIT
hopstack?

CITE
Schnabel [99]
%http://www.sciencedirect.com/science/article/pii/S0167819103000139




\section{My Algorithm}

\subsection{Problem}

We consider problems of the form

\begin{align*}
\min_x & f(x) \\
 & g(x) \le 0 \\
 & h(x) = 0
\end{align*}

where $f : R^n \to R$, $g : R^{n} \to R^{m_1}$ and $h : R^{n} \to R^{m_2}$.
We assume that all functions are derivative-free: all functions $f,g,h$ are evaluated by a single call to a black box function $d(x) = (f(x), g(x)^T, h(x)^T)^T$.



\subsection{First approach}
We began with a line search filter method, however we found that this had several drawbacks:

\begin{itemize}
\item Trust regions arise naturally within derivative-free algorithms
\item Line search algorithms exploit how much easier finding a descent direction is than solving the trust region subproblem, but this saved computation is not as useful in DFO
\item As the algorithm backtracks on the step length, the trust region must be reduced, as the models are accurate over a region rather than at a single point
\end{itemize}

\subsection{Algorithm in other paper}

We work within a trust region, sequential quadratic programming framework that uses a filter method introduced by Fletcher.

We first compute an interpolation set poised for regressing a set of model functions, which we choose to be quadratic functions.
Although we have function values for enough points to construct model functions of the same order as used for the objective, we model the feasible with only the linear constraints.

The core of the algorithm revolves around

\subsubsection{Criticality Measure}
In order to construct stopping criteria, we introduce a criticality measure $\chi$ which goes to zero as the iterates approach a first order critical point.

This is defined as
\begin{align*}
\chi & = & |\min_t \langle g_k + H_kn_k, t\rangle| \\
& A_{eq}t &=& \; 0 \\
& c_{ineq} + A_{ineq}t &\le& \; 0 \\
& \| t \| &\le& \; 1
\end{align*}



\subsubsection{Step decomposition}
At iteration $k$, we can decompose the step $s_k$ into a normal step $n_k$ intended to decrease constraint violation and a tangential step $t_k$ intended to reduce the objective.
The step $n_k$ projects the current iterate onto the feasible region.
Currently, we project $x_k$ onto only the linear model of the feasible region.
We require that the the computation of the normal step, which solves:

\begin{align*}
n_k &=& \argmin_n           & \|n\|^2 \\
s.t. \quad & c_{eq} + A_{eq}n     &=&\; 0 \\
     & c_{ineq} + A_{ineq}n &\le& \; 0  \\
     & \| n \|^2            &\le& \; \Delta_k^2
\end{align*}

In addition to this program having a feasible point, we need to know that there is enough space for us to provide sufficient decrease within the tangential step. This means that we require the stronger condition that

$$\|n\|\le \kappa_{\Delta} \Delta_k \min \{1, \kappa_{\mu}\Delta_k^{\mu}\}$$

If this condition is satisfied, then we say that the program is \emph{compatible}. We are then able to compute a tangential step $t_k$:

\begin{align*}
t_k &=& \argmin_t           & (g_n+H_kn_k)^Tt + \frac 1 2 t^T H_k t \\
s.t. \quad & c_{eq} + A_{eq}t	&=& \; 0 \\
     & c_{ineq} + A_{ineq}t	&\le& \; 0  \\
     & \| n_k + t_k \|^2 		&\le& \; \Delta_k ^2
\end{align*}

\begin{itemize}
\item quadratic information contained in $H_k$
\item $H_k$ is the hessian of the lagrangian, as computed by using KKT the matrix
\item This program is a shifted version of another
\end{itemize}

%\subsubsection{Compatibility}


\subsubsection{The filter}

The filter is a method used to ensure convergence to a feasible point.
It works by ensuring that all new iterates are nondominated with respect to all previous iterates when the problem is viewed as a multi-criteria optimization problem $\min (\theta, f)$. However, simply ensuring that new points are nondominated does not provide sufficient progress, we must ensure that the objective decreases by a greater amount when the current iterate is far from the feasible region:

\[
\theta(x_k) \le (1-\gamma_{\theta})\theta_i
\]
or
\[
f(x_k) \le f_i -\gamma_{\theta}\theta_i
\]

for all $(\theta_i, f_i)$ that are currently in the filter.
When this condition is satisfied, we say that the new iterate $x_k$ is admissible to the filter.

\subsubsection{$f$-type versus $\theta$-type}

When steps decrease $f$ significantly more than $theta$ the steps are considered $f$ type.
$theta$-type steps are steps that significantly reduce the constraint violation.
I thought that the paper I read about the convergence rate said they introduced a new criteria to this that made it more efficient.

\subsubsection{Restoration Step}

The goal of the feasibility restoration step is to find a new iterate and trust region radius that allows the current iterate to be compatible.

While performing the restoration step, we place constraints in the objective by minimizing the squared norm of $\theta$.
We take one step to minimize the quadratic model unconstrained optimization problem, and update the trust region based on the new function value.

It is possible that the restoration step is unsuccessful if the iterates approach an infeasible local minimum of the constraints.
In this case, the algorithm will fail to find a feasible local minimum, and will need to be restarted.


\subsection{When we use derivative-free methods}

\subsubsection{The change in the criticality measure}
One issue with applying the original algorithm within a DFO context was that the trust region radius is not required to go to zero.
However, within DFO we must also require that the trust region goes to zero as we approach a stationary point. One way of ensuring this is to decrease the trust region radius when the current step lies well within the trust region radius. However, a better approach may be to introduce a tolerance on the criticality measure and decrease the trust region whenever the criticality falls below the threshold.


\subsection{Algorithm Description}

\subsubsection{Simplified version draft}

\begin{itemize}
\item compute model functions and hessian of the lagrangian
\item compute criticality measure
\item if feasible and critical, then decrease trust region radius or return
\item compute normal step
\item check compatibility, restoring feasibility if necessary and return to step 1
\item compute tangential step
\item check for sufficient reduction in the model function, adding the current iterate to the filter and returning to step 1 if necessary
\item evaluate the function and constraints at the trial point
\item check compatibility to the filter, decreasing the trust region radius and returning to step 1 if necessary
\item compute $\rho$, and accept the trial point or reject and decrease the radius
\end{itemize}

\subsubsection{Psuedo code}

\FloatBarrier
\begin{algorithm}
\caption{Filter Trust Region Search}\label{linesearch1}
\begin{algorithmic}[1]
\Procedure{trust region filter}{}
\State{initialize}
\State{$k=0$}
\State{choose an $x_0$}

\While {$k < maxit$}
	\Label \texttt{main loop:}

	\State{ensure poisedness, possibly adding points to the model}
	\State{Compute $m_k, g_k=\nabla m_k(x_k), c_k, A_k, f_k=f(x_k), \mathcal {A}, \theta_k$}
	\State{Solve:}
	\State \begin{varwidth}[t]{\linewidth}
		\hspace{3cm}$\nabla^2m_k(x_k)d + A_k^T\lambda = g_k$ \par
		\hspace{3cm}$A_kd\hspace{1cm}              = c_k$
	\end{varwidth}
	\State{$H_k \gets \nabla^2 m_k(x_k) + \sum_i \lambda_i \nabla^2 {c_{i}}_k$}


	\State{$\chi_k \gets |\min_t \{\langle g_k + H_kn_k, t\rangle | A_{eq}t = 0 \wedge c_{ineq} + A_{ineq}t \le 0 \wedge \| t \| \le 1\}|$}

 	\If {constraint violation $=0 \wedge \chi=0$}
		\If {$tol < \Delta_k$}
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State{$k \gets k+1$}
			\State \Goto \texttt{main loop}
		\EndIf
		\textbf{success}
	\EndIf


	\State{$n_k \gets \argmin_n \{\|n\|^2 | c_{eq} + A_{eq}n = 0 \wedge c_{ineq} + A_{ineq}n \le 0 \wedge \| n \|^2 \le \Delta_k\}^2$}

	\If {Feasible region $\ne \emptyset \wedge \|n\|\le \kappa_{\Delta} \Delta_k \min \{1, \kappa_{\mu}\Delta_k^{\mu}\}$}
		\State{$t_k \gets \argmin_t \{ (g_n+H_kn_k)^Tt + \frac 1 2 t^T H_k t | c_{eq} + A_{eq}t = 0 \wedge c_{ineq} + A_{ineq}t \le 0 \wedge \| s \| \le \Delta_k\}$}
		\State{$s_k \gets t_k + n_k$}

		\If {$m_k(x_k) - m_k(x_k+s_k) \ge \kappa_{\theta} \theta_k^{\psi}$}
			\State{add $x_k$ to filter}
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State \Goto \texttt{main loop}
		\EndIf

		\State{// Here we evaluate new $c$ and $f$ at $x_k + s_k$}
		\If {$x_k + s_k$ is acceptable: $\theta(x_k+s_k)\le(1-\gamma_{\theta})\theta' \vee f(x_k+s_k) \le f' - \gamma_{\theta}\theta' \forall (f', \theta') \in $ Filter}
			\State{$\rho = \frac{f(x_k)-f(x_k+s_k)}{m_k(x_k)-m_k(x_k+s_k)}$}
			\If {$\rho < \eta_1$}
				\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
				\State{$k \gets k+1$}
				\State \Goto \texttt{main loop}
			\ElsIf {$\rho > \eta_2$}
				\If {$\|s\| < \frac{\Delta_k}{2}$}
					\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
				\Else
					\State{increase $\Delta$: $\Delta_{k+1} \gets$ some $ \in [\Delta_k, \gamma_2 \Delta_k]$}
				\EndIf

			\EndIf
			\State{$x_{k+1} \gets x_k + s_k$}
		\Else
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State{$k \gets k+1$}
			\State \Goto \texttt{main loop}
		\EndIf
	\Else
		\State{add $x_k$ to filter}
		\State{compute new $r$ (restoration step) and $\Delta$}
		\If{impossible to restore}
			\textbf{fail}
		\EndIf
		\State{$x_{k+1} \gets x_k + r$}
	\EndIf
	\State{$k \gets k+1$}
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\subsection{Generalizations}

As this method was also discovered by () we have also considered possible extensions.

\begin{itemize}
\item The authors only considered linear models, we can use quadratic models
\item We can generalize (relax) the possible steps by only requiring only that new iterates are admissable with respect to a multi objective filter of the form $(f(x), \|g_1(x)\|^2, \ldots \|g_{m_1}(x)\|^2, \|h_1(x)\|, \ldots, \|h_{m_2}\|)$.
\item We could relax the condition that all iterates have to remain feasible with respect to all constraints except $y = d(z)$.
\end{itemize}

\subsection{Pictures of convergence}

Here we give an example of how this algorithm works on a test problem.

\section{Comparison to other libraries}
\section{Testing Progress}
In order to compare the algorithms we develop, we will use the

\begin{itemize}
\item SDPEN
A Sequential Penalty Derivative-free Method for Nonlinear
Constrained Optimization problems
Copyright (C) 2011  G.Liuzzi, S.Lucidi, M.Sciandrone
\item DAKOTA/PATTERN
\item tomlab lgo, ego?
\item nomad
\end{itemize}

on test sets

\begin{itemize}
\item dfovec
\item cute/st
\end{itemize}

http://thales.cheme.cmu.edu/dfo/comparison/dfo.pdf


unconstrained has several more:
fmincon


This currently has no place in the paper:

Within heuristics behind
Applying DFO techniques to algorithms that currently use random sampling to tune parameters
Machine learning techniques are a hot topic, and some techniques have very complicated objectives for which derivative information is not known.



\newpage

\bibliography{proposal}
\bibliographystyle{ieeetr}


\end{document}
