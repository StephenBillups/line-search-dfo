\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{varwidth}% http://ctan.org/pkg/varwidth
\usepackage{xspace}
\usepackage{placeins}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsfonts}

%\theoremstyle{definition}
%\newtheorem*{dfn}{A Reasonable Definition}


\DeclareMathOperator*{\argmin}{arg\,min}


\title{Possible Future Research}
\author{Trever Hallock}
%\institute{CU Denver}
%\date{January 6, 2012}
% Remove the % from the previous line and change the date if you want a particular date to be displayed; otherwise, today's date is displayed by default.

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother


\begin{document}


\algnewcommand{\algorithmicgoto}{\textbf{go to}}%
\algnewcommand{\Goto}{\algorithmicgoto\xspace}%
\algnewcommand{\Label}{\State\unskip}


\maketitle


\tableofcontents

\section{Introduction}

This paper will discuss research topics for derivative free optimization.
It begins with an introduction to the context and goals of derivative free optimization (DFO) supplemented by some of the recent advancement made in the field.
It then details several future research directions and one in particular (filter methods) that has been studied.
The focus is on local search algorithms for constrained derivative free optimization.


\section{Derivative-free Mentality}

Derivative free optimization is motivated by programs in which derivative information is unknown, deceptive or otherwise impractical to compute.
For example, this can arise when the objective is the result of a simulation that does not admit automatic differentiation due to copyrights or is too expensive for repeated calls within a finite difference framework.
The general strategy is to converge to a first or second order critical point while evaluating the function as few times as possible.

Another branch of DFO is concerned with noisy function evaluation.
Noisy functions can be categorized as either deterministic or random.
Deterministic means that the genuine objective is not always evaluated accurately, but the measurement error (?) will not change across multiple function calls at the same point.
Random means that each point in the domain is associated with a distribution of possible values the objective may return.

\subsubsection{Derivatives are convenient}

The lack of derivative information means that DFO methods are at a disadvantage when compared to their counterparts in nonlinear optimization.
First and second derivative information is explicit in algorithms with quadratic convergence such as Newton's method.
They are also present in conditions for convergence results such as Wolf's, Armijo or Goldstien for line search methods.
Additionally, stopping criteria usually involve a criticality test involving derivatives.

\subsubsection{Problem statement}

In more detail, DFO methods consider nonlinear, constrained optimization problems of the form

\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}} & & f(x, S(x)) \\
& \text{subject to} & & g_i(x, S(x)) \leq 0, \; i = 1, \ldots, m_{\mathcal{I}} \\
& & & h_i(x, S(x)) = 0, \; i = 1, \ldots, m_{\mathcal{E}}
\end{aligned}
\end{equation*}

which give rise to different classes of derivative-free optimization based on properties of $f : \mathbb{R}^n \to \mathbb{R}$, $g_i \mathbb{R} \to \mathbb{R}$, and $h_i \mathbb{R} \to \mathbb{R}$.
As discussed, these functions may be noisy, either deterministically or randomly, although this paper does not deal with this case.
Derivatives of $g_i$ and $h_i$ may be known, in which case the objective is the only derivative free function.
One common such case is to include bound constraints of the form $b_{L} \le x \le b_{U}$ for some $b_{L} < b_{U}$, which gives rise to Box Constrained DFO (BCDFO).
If the constraints can be evaluated at points outside the feasible region, the constraints are called relaxable constraints.
We consider the case where no derivative information of $g_i$ and $h_i$ are known, for example if they are also output from a simulation used to evaluate the objective.
This means that each call to the objective gives values of the constraints as well, and vica versa.
However, although derivatives are not known, we do assume that $f$, $g_i$, and $h_i$ are all continuously twice differentiable.
Some problems additionally contain ``hidden" constraints which are not explicit in the model but merely result in a notification that the objective could not be evaluated at the requested point.
This may mean that it is not possible to tell how close to a ``hidden" constraint the current iterate lies.
Again, we only consider only local search algorithms for this problem that seek a first or second order stationary point.

Many DFO methods simply let $f(x,S(x)) = S(x)$.

A slightly more general form is presented by \cite{DUMMY:Biegler} and given here:
\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}} & & f(x, y, z) \\
& \text{subject to} & & g_i(x, y, z) \leq 0 \quad \forall i = 1, \ldots, m_1 \\
& & & h_i(x, y, z) = 0, \quad \forall i = 1 \ldots, m_2 \\
& & & y = d(z) \;  \\
\end{aligned}
\end{equation*}

Biegler assumes that the dimension of $u$ is small compared to the dimension of $x$ and $z$.

\subsection{Approaches}

In this section we discuss several of the approaches taken when derivative information is not known.

\subsubsection{Finite difference methods}
Finite difference methods can be used to approximate the derivative of a function $f$.
A common approximation is given by $\nabla f(x) \approx (\frac{f(x+he_i) - f(x-he_i)}{2h})_i$ for some small $h$.
This may work well but can have issues with unlucky iterates.
The number of function evaluations tends to grow large with the dimension and number of iterations the algorithm performs as information is only gathered near the current iterate when $h$ is small (which is required for accurate derivatives).
Because of the large number of function evaluations required for finite difference schemes, these can be unusable.

CITE


\subsubsection{Direct search methods}

Another approach is to use direct search methods that do not explicitly estimate the derivative but evaluate the objective on a pattern or other structure to find a descent direction.
Examples of this include Coordinate descent and other pattern based search methods.
One of the most popular direct search method is Nelder Mead (it is implemented in fminsearch in matlab) although it is proven to not converge in (synonym: bad) cases.
These methods can be robust but ignore information because they do not use derivatives.


(0th derivative)

\subsubsection{Model based methods}
In this paper, we are concerned with model based methods that minimize a model used to approximation of the unknown objective (by regression or kriging or ...).
The algorithm can then use derivative information from the model to find a descent direction.
These methods require an adequate geometry of sampled points to ensure convergence.

More specifically, regression based methods require that set the function is evaluated at must be $\Lambda$ poised for a fixed constant $\Lambda$.
This ensures that the Vandermode matrix used to find the coefficients used to express the model function in terms of a basis of Lagrange polynomials is well conditioned.
$\dots$.




\subsection{The Derivative Free Trust Region Method}

The overall description of the trust region framework is that a set of poised points are chosen for some radius $\Delta$ about the current iterate.
The objective/constraint are then evaluated at these point to construct a model function as a linear combination of some set of basis functions.
Next, the model is minimized over this trust region to and the minimum becomes the trial point.
The objective is evaluated at the trail point and a measure of reduction $\rho$ is computed.
If $\rho$ implies that sufficient reduction has been made, the trial point is accepted as the new iterate.
Otherwise, the trust region is reduced.

\subsubsection{More details}

\begin{enumerate}
	\item Define $m_k(x) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x-x^{(k)}) + \frac 1 2 (x-x^{(k)})^T\nabla^2f(x^{(k)})(x-x^{(k)})$
	\begin{itemize}
		\item $\nabla f(x^{(k)})$ and $\nabla^2 f(x^{(k)})$ must be approximated
		\item There are geometric properties of the sample set that must be satisfied
	\end{itemize}
	\item if $\nabla m_k(x) < t$ stop
	\item Solve the Trust region subproblem: $s^{(k)} = \argmin_{s\in B_(x^{(k)}, \Delta_k)} m_k(x^{(k)} + s)$
	\item Test for improvement
	\begin{itemize}
		\item $\rho_k = \frac{f(x^{(k)}) - f(x^{(k)}+s^{(k)})}{m_k(x^{(k)}) - m_k(x^{(k)}+s^{(k)})}$
		\item If $\rho$ is small, $x^{(k+1)}=x^{(k)}$ (reject) and decrease radius
		\item If $\rho$ is intermediate, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and decrease radius
		\item If $\rho$ is large, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and increase radius
	\end{itemize}
\end{enumerate}


%\begin{equation*}
%\begin{aligned}
%& \underset{x}{\text{minimize}} & & f(x, S(x)) \\
%& \text{subject to} & & g_i(x) \leq 0, \; i = 1, \ldots, m \\
%& & & h_i(x) = 0, \; i = 1, \ldots, n
%\end{aligned}
%\end{equation*}


\section{Literature Review}

The original filter method was proposed by Gould in \cite{DUMMY:original_filter}. The motivation for the filter method was that the algorithm does not need to tune any parameters as in penalty or merit methods.

A recent paper from September 2016 \cite{DUMMY:Biegler} implements a derivative-free trust region filter method for solving the general program

\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}} & & f(x, y, z) \\
& \text{subject to} & & g_i(x, y, z) \leq 0 \quad \forall i = 1, \ldots, m_1 \\
& & & h_i(x, y, z) = 0, \quad \forall i = 1 \ldots, m_2 \\
& & & y = d(z) \;  \\
\end{aligned}
\end{equation*}
where $d$ is a black box function while $f$, $g$ and $h$ are glass box functions.
This is more general than the algorithm we consider as it allows for the objective to depend on other glass box functions of the input and multiple outputs of the black box function. The authors compare their algorithm to finite difference methods as well as kriging on three different applications from Chemistry. Within the algorithm, the current iterate is always feasible with respect all inequalities except for the constraint $y=d(z)$.





In the 2006 paper \cite{DUMMY:Fletcher} Fletcher reviews how filter methods have developed. The only references to derivative-free versions of the filter method are those applied to pattern based (direct) methods. However, the authors outline trust region filter methods along with several other variants.

In Brekelman's paper \cite{DUMMY:Brekelman}, a trust region filter method is developed to minimize function evaluations by constructing linear model functions. This paper also uses experimental designs to choose following iterates.


Within \cite{DUMMY:linesearch_global} and \cite{DUMMY:linesearch_local} Biegler uses a filter method to ensure global convergence within a line search framework. We experimented with this before deciding combining line search with the derivative-free trust region approach was not a natural approach. (There have been attempts to employ both of these frameworks in \cite{DUMMY:CombineTrustAndLine}.)


Within  \cite{DUMMY:intro_book} derivative-free methods are developed in detail. This contains a good explanation of ensuring geometry of the current set with poisedness for unconstrained problems and also covers other derivative-free methods including direct-search and line search.


Within \cite{DUMMY:trust_funnel_dfo} Toint generalizes the filter method with the notion of a trust funnel. This is for glass box functions as it does not include any derivative-free methods.


Within \cite{DUMMY:sqp_filter} a sequential quadratic programming method is applied to the filter method?


Colson has also applied filter techniques to derivative-free optimization in his 2004 Ph.D. thesis \cite{Colson2004}. This focuses on bilevel programming.


I based my algorithm of the trust region filter method described in:

GLOBAL CONVERGENCE OF A TRUST-REGION SQP-FILTER
ALGORITHM FOR GENERAL NONLINEAR PROGRAMMING∗
ROGER FLETCHER†, NICHOLAS I. M. GOULD‡, SVEN LEYFFER†,
PHILIPPE L. TOINT§, AND ANDREAS WACHTER ¨ ¶
I need to find a bibtex reference of this paper to put in the bibliography.



% In order to give this talk as is, I need to be prepared to talk about
% and preferably have optional slides for
% Coordinate descent, Nelder mead (not implicit filtering http://www4.ncsu.edu/~ctk/imfil.pdf)
% Wolf, Armijo, or Goldstein
% trust region subproblem
% know values of \eta_1, \eta_2
% interpolation may involve frebenius norm/enforcing sparsity
% http://www.sciencedirect.com/science/article/pii/S0167819103000139
% Should use the word black box early on

the two additional review papers.



\section{Possible future work}

\subsection{Future Directions}

Possible future research directions fall among the follows topics:

\begin{itemize}
\item Convert classical NLP Algorithms to DFO
\item Consider different DFO Goals
\item Parallelize DFO Algorithms
\item Explore different model functions
\item Impose structure
\end{itemize}


\subsection{NLP Methods}

Possible future work includes converting nonlinear algorithms to a derivative free context.
We outline several nonlinear programming algorithms, some of which others have made progress in translating to a DFO context.

\begin{itemize}
	\item Line search (CITE)
	\item Filter method
	\begin{itemize}
		\item The first filter methods in DFO were introduced in 2004 CITE within the context of pattern searches
		\item Since then, progress has also been made in a trust region framework CITE, CITE, CITE
	\end{itemize}
	\item Active Set (CITE)
	\item Augmented Lagrangian (CITE)
	\item Penalty
	\begin{itemize}
		\item Abramson \& Audet, 2006
		\item Abramson et al. 2009c
		\item audet et al. 2008b
		\item Audet \& Dennis, 2006
		\item sequential penalty merit functions Liuzzi et al., 2010
		\item smoothed exact $l-\infty$ penalty function Liuzzi \& Lucidi, 2009
		\item exact penalty merit function Fasano, Liuzzi, Lucidi, \& Rinalsdi, 2014; Gratton \& Vicente, 2014
	\end{itemize}
	\item Progressive Barrier
	\begin{itemize}
		\item Audet \& Dennis, 2009
	\end{itemize}
	\item Interior Point (CITE)
\end{itemize}

The general approach is to replace derivatives of $f$, $g_i$, $h_i$ with derivatives of the model function.
Then other aspects of the algorithm may have to be changed in order to ensure convergence.
I have considered line search methods as well as Filter methods which are discussed below.





\subsection{Other directions}
\subsubsection{Goals}
We could consider an objective function with a evaluation runtime that varies with $x$.
For example, in some problems from PDE's the runtime of a simulation may involve the ``stiffness" of $\ldots$.
In these situations, it may be advisable to consider the runtime cost of the function when evaluating new points: choosing points that remain poised with respect to the geometry but allow for faster evaluation.

%We could minimize the number of future function evaluations given a stochastic model of what the new function values could be.

% We could consider the following bicriteria optimization problem:
%
% \begin{equation*}
% \begin{aligned}
% & \underset{x}{\text{minimize}} & & (f(x, S(x)), \sum_i r(x_i)) \\
% & \text{subject to} & & g_i(x) \leq 0, \; i = 1, \ldots, m \\
% & & & h_i(x) = 0, \; i = 1, \ldots, n
% \end{aligned}
% \end{equation*}
%
% \begin{itemize}
% \item $x_i$ is the generated points where $f(x)$ are evaluated
% \item $r(x)$ is the runtime of evaluating $f(x)$, or possibly worst case runtime
% \item This may require applications with smooth variability in runtime, so that a models can be constructed or derivatives can be taken.
% \item In order to provide appreciable improvement, we also desire significant variation in $r(x)$.
% \end{itemize}



\subsubsection{Parallelization}

Parallelization is another topic receiving allot of attention.
Modern super computer design can be exploited in several different ways.
These include parallelization of the function and/or derivative evaluations within the algorithm or of the linear algebra kernels.
However, the more relevant is from modification the basic algroithms which increase the degree of intrinsic parallelism. This can come from performing multiple function and/or derivative evaluations.


CITE
Schnabel [99]
%http://www.sciencedirect.com/science/article/pii/S0167819103000139


\subsubsection{Miscellaneous}

\begin{itemize}
\item Model Functions
	Radial basis functions
\item Problem Structure
	Specify the structure of $f$, $g_i$, or $h_i$. For example, work has been done when these functions take the form of a least squares problem. CITE
\item Machine Learning
	Applying DFO techniques to algorithms that currently use random sampling to tune parameters
\end{itemize}

Of current interest:
\begin{itemize}
\item Any time algorithms maintain feasibility so that the algorithm can be stopped at any time to yield feasible guess. The longer the algorithm is run, the better the returned value is, until optimality is reached.
\item Machine learning techniques are a hot topic, and some techniques have very complicated objectives for which derivative information is not known.
\item Constraints that leave a narrow feasible region near the critical point. (Harming lambda poisedness.) One approach may be to redefine the poisedness.
\end{itemize}





\section{My Algorithm}

\subsection{Problem}

We consider problems of the form

\begin{align*}
\min_x & f(x) \\
 & g(x) \le 0 \\
 & h(x) = 0
\end{align*}

where $f : R^n \to R$, $g : R^{n} \to R^{m_1}$ and $h : R^{n} \to R^{m_2}$.
We assume that all functions are derivative-free: all functions $f,g,h$ are evaluated by a single call to a black box function $d(x) = (f(x), g(x)^T, h(x)^T)^T$.



\subsection{First approach}
We began with a line search filter method, however we found that this had several drawbacks:

\begin{itemize}
\item Trust regions arise naturally within derivative-free algorithms
\item Line search algorithms exploit how much easier finding a descent direction is than solving the trust region subproblem, but this saved computation is not as useful in DFO
\item As the algorithm backtracks on the step length, the trust region must be reduced, as the models are accurate over a region rather than at a single point
\end{itemize}

\subsection{Algorithm in other paper}

We work within a trust region, sequential quadratic programming framework that uses a filter method introduced by Fletcher.

We first compute an interpolation set poised for regressing a set of model functions, which we choose to be quadratic functions.
Although we have function values for enough points to construct model functions of the same order as used for the objective, we model the feasible with only the linear constraints.

The core of the algorithm revolves around

\subsubsection{Criticality Measure}
In order to construct stopping criteria, we introduce a criticality measure $\chi$ which goes to zero as the iterates approach a first order critical point.

This is defined as
\begin{align*}
\chi & = & |\min_t \langle g_k + H_kn_k, t\rangle| \\
& A_{eq}t &=& \; 0 \\
& c_{ineq} + A_{ineq}t &\le& \; 0 \\
& \| t \| &\le& \; 1
\end{align*}



\subsubsection{Step decomposition}
At iteration $k$, we can decompose the step $s_k$ into a normal step $n_k$ intended to decrease constraint violation and a tangential step $t_k$ intended to reduce the objective.
The step $n_k$ projects the current iterate onto the feasible region.
Currently, we project $x_k$ onto only the linear model of the feasible region.
We require that the the computation of the normal step, which solves:

\begin{align*}
n_k &=& \argmin_n           & \|n\|^2 \\
s.t. \quad & c_{eq} + A_{eq}n     &=&\; 0 \\
     & c_{ineq} + A_{ineq}n &\le& \; 0  \\
     & \| n \|^2            &\le& \; \Delta_k^2
\end{align*}

In addition to this program having a feasible point, we need to know that there is enough space for us to provide sufficient decrease within the tangential step. This means that we require the stronger condition that

$$\|n\|\le \kappa_{\Delta} \Delta_k \min \{1, \kappa_{\mu}\Delta_k^{\mu}\}$$

If this condition is satisfied, then we say that the program is \emph{compatible}. We are then able to compute a tangential step $t_k$:

\begin{align*}
t_k &=& \argmin_t           & (g_n+H_kn_k)^Tt + \frac 1 2 t^T H_k t \\
s.t. \quad & c_{eq} + A_{eq}t	&=& \; 0 \\
     & c_{ineq} + A_{ineq}t	&\le& \; 0  \\
     & \| n_k + t_k \|^2 		&\le& \; \Delta_k ^2
\end{align*}

\begin{itemize}
\item quadratic information contained in $H_k$
\item $H_k$ is the hessian of the lagrangian, as computed by using KKT the matrix
\item This program is a shifted version of another
\end{itemize}

%\subsubsection{Compatibility}


\subsubsection{The filter}

The filter is a method used to ensure convergence to a feasible point.
It works by ensuring that all new iterates are nondominated with respect to all previous iterates when the problem is viewed as a multi-criteria optimization problem $\min (\theta, f)$. However, simply ensuring that new points are nondominated does not provide sufficient progress, we must ensure that the objective decreases by a greater amount when the current iterate is far from the feasible region:

\[
\theta(x_k) \le (1-\gamma_{\theta})\theta_i
\]
or
\[
f(x_k) \le f_i -\gamma_{\theta}\theta_i
\]

for all $(\theta_i, f_i)$ that are currently in the filter.
When this condition is satisfied, we say that the new iterate $x_k$ is admissible to the filter.

\subsubsection{$f$-type versus $\theta$-type}

When steps decrease $f$ significantly more than $theta$ the steps are considered $f$ type.
$theta$-type steps are steps that significantly reduce the constraint violation.
I thought that the paper I read about the convergence rate said they introduced a new criteria to this that made it more efficient.

\subsubsection{Restoration Step}

The goal of the feasibility restoration step is to find a new iterate and trust region radius that allows the current iterate to be compatible.

While performing the restoration step, we place constraints in the objective by minimizing the squared norm of $\theta$.
We take one step to minimize the quadratic model unconstrained optimization problem, and update the trust region based on the new function value.

It is possible that the restoration step is unsuccessful if the iterates approach an infeasible local minimum of the constraints.
In this case, the algorithm will fail to find a feasible local minimum, and will need to be restarted.


\subsection{When we use derivative-free methods}

\subsubsection{The change in the criticality measure}
One issue with applying the original algorithm within a DFO context was that the trust region radius is not required to go to zero. However, within DFO we must also require that the trust region goes to zero as we approach a stationary point. One way of ensuring this is to decrease the trust region radius when the current step lies well within the trust region radius. However, a better approach may be to introduce a tolerance on the criticality measure and decrease the trust region whenever the criticality falls below the threshold.


\subsection{Algorithm Description}

\subsubsection{Simplified version draft}

\begin{itemize}
\item compute model functions and hessian of the lagrangian
\item compute criticality measure
\item if feasible and critical, then decrease trust region radius or return
\item compute normal step
\item check compatibility, restoring feasibility if necessary and return to step 1
\item compute tangential step
\item check for sufficient reduction in the model function, adding the current iterate to the filter and returning to step 1 if necessary
\item evaluate the function and constraints at the trial point
\item check compatibility to the filter, decreasing the trust region radius and returning to step 1 if necessary
\item compute $\rho$, and accept the trial point or reject and decrease the radius
\end{itemize}

\subsubsection{Psuedo code}

\FloatBarrier
\begin{algorithm}
\caption{Filter Trust Region Search}\label{linesearch1}
\begin{algorithmic}[1]
\Procedure{trust region filter}{}
\State{initialize}
\State{$k=0$}
\State{choose an $x_0$}

\While {$k < maxit$}
	\Label \texttt{main loop:}

	\State{ensure poisedness, possibly adding points to the model}
	\State{Compute $m_k, g_k=\nabla m_k(x_k), c_k, A_k, f_k=f(x_k), \mathcal {A}, \theta_k$}
	\State{Solve:}
	\State \begin{varwidth}[t]{\linewidth}
		\hspace{3cm}$\nabla^2m_k(x_k)d + A_k^T\lambda = g_k$ \par
		\hspace{3cm}$A_kd\hspace{1cm}              = c_k$
	\end{varwidth}
	\State{$H_k \gets \nabla^2 m_k(x_k) + \sum_i \lambda_i \nabla^2 {c_{i}}_k$}


	\State{$\chi_k \gets |\min_t \{\langle g_k + H_kn_k, t\rangle | A_{eq}t = 0 \wedge c_{ineq} + A_{ineq}t \le 0 \wedge \| t \| \le 1\}|$}

 	\If {constraint violation $=0 \wedge \chi=0$}
		\If {$tol < \Delta_k$}
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State{$k \gets k+1$}
			\State \Goto \texttt{main loop}
		\EndIf
		\textbf{success}
	\EndIf


	\State{$n_k \gets \argmin_n \{\|n\|^2 | c_{eq} + A_{eq}n = 0 \wedge c_{ineq} + A_{ineq}n \le 0 \wedge \| n \|^2 \le \Delta_k\}^2$}

	\If {Feasible region $\ne \emptyset \wedge \|n\|\le \kappa_{\Delta} \Delta_k \min \{1, \kappa_{\mu}\Delta_k^{\mu}\}$}
		\State{$t_k \gets \argmin_t \{ (g_n+H_kn_k)^Tt + \frac 1 2 t^T H_k t | c_{eq} + A_{eq}t = 0 \wedge c_{ineq} + A_{ineq}t \le 0 \wedge \| s \| \le \Delta_k\}$}
		\State{$s_k \gets t_k + n_k$}

		\If {$m_k(x_k) - m_k(x_k+s_k) \ge \kappa_{\theta} \theta_k^{\psi}$}
			\State{add $x_k$ to filter}
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State \Goto \texttt{main loop}
		\EndIf

		\State{// Here we evaluate new $c$ and $f$ at $x_k + s_k$}
		\If {$x_k + s_k$ is acceptable: $\theta(x_k+s_k)\le(1-\gamma_{\theta})\theta' \vee f(x_k+s_k) \le f' - \gamma_{\theta}\theta' \forall (f', \theta') \in $ Filter}
			\State{$\rho = \frac{f(x_k)-f(x_k+s_k)}{m_k(x_k)-m_k(x_k+s_k)}$}
			\If {$\rho < \eta_1$}
				\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
				\State{$k \gets k+1$}
				\State \Goto \texttt{main loop}
			\ElsIf {$\rho > \eta_2$}
				\If {$\|s\| < \frac{\Delta_k}{2}$}
					\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
				\Else
					\State{increase $\Delta$: $\Delta_{k+1} \gets$ some $ \in [\Delta_k, \gamma_2 \Delta_k]$}
				\EndIf

			\EndIf
			\State{$x_{k+1} \gets x_k + s_k$}
		\Else
			\State{reduce $\Delta$: $\Delta_{k+1} \gets $ some $\in [\gamma_0 \Delta_k, \gamma_1 \Delta_k]$}
			\State{$k \gets k+1$}
			\State \Goto \texttt{main loop}
		\EndIf
	\Else
		\State{add $x_k$ to filter}
		\State{compute new $r$ (restoration step) and $\Delta$}
		\If{impossible to restore}
			\textbf{fail}
		\EndIf
		\State{$x_{k+1} \gets x_k + r$}
	\EndIf
	\State{$k \gets k+1$}
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\subsection{Generalizations}

As this method was also discovered by () we have also considered possible extensions.

\begin{itemize}
\item The authors only considered linear models, we can use quadratic models
\item We can generalize (relax) the possible steps by only requiring only that new iterates are admissable with respect to a multi objective filter of the form $(f(x), \|g_1(x)\|^2, \ldots \|g_{m_1}(x)\|^2, \|h_1(x)\|, \ldots, \|h_{m_2}\|)$.
\item We could relax the condition that all iterates have to remain feasible with respect to all constraints except $y = d(z)$.
\end{itemize}

\section{Pictures of convergence}
\section{Comparison to other libraries}




\newpage

\bibliography{proposal}
\bibliographystyle{ieeetr}


\end{document}
