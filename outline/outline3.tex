\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{varwidth}% http://ctan.org/pkg/varwidth
\usepackage{xspace}
\usepackage{placeins}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsfonts}
%\theoremstyle{definition}
%\newtheorem*{dfn}{A Reasonable Definition}



\DeclareMathOperator*{\argmin}{arg\,min}


\title{Derivative Free Model-Based Methods for Local Constrained Optimization}
\author{Trever Hallock}
%\institute{CU Denver}
%\date{January 6, 2012}
% Remove the % from the previous line and change the date if you want a particular date to be displayed; otherwise, today's date is displayed by default.

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\let\oldref\ref
\renewcommand{\ref}[1]{(\oldref{#1})}

\begin{document}


\algnewcommand{\algorithmicgoto}{\textbf{go to}}%
\algnewcommand{\Goto}{\algorithmicgoto\xspace}%
\algnewcommand{\Label}{\State\unskip}


\maketitle


\tableofcontents

\newpage

\section{Introduction}

This paper will discuss research topics for my research in derivative free optimization (DFO).
It begins with an introduction to derivative free optimization supplemented by some of the recent advancements.
It then details several future research directions and one in particular (filter methods) that has been studied.
The focus is on model-based trust region algorithms for local search within constrained derivative free optimization.
Specifically, we are interested in problems with narrow feasible regions.
Narrow constraints introduce numerical instability and make it hard to to form approximates for a function.
We have experimented with several strategies for handling hidden constraints.
These introduce numerical instability and make it difficult to form model functions. 

\subsection{Introduction to Derivative Free Optimization}
\subsubsection{What it is}

Derivative free optimization refers to mathematical programs in which derivative information is not explicitly available.
Derivatives cannot be calculated directly, and function evaluations are computationally expensive.
%The number of function evaluations can grow when approximating derivatives that are not given explicitly.
One of the primary goals within derivative free optimization is to solve programs while avoiding as many expensive function evaluations as possible.


\subsubsection{Problem formulations}

This proposal aims to develop derivative-free algorithms for solving constrained nonlinear optimization problems of the form
\[ \begin{array}{ccl} \min & f(x) \\
\mbox{subject to} & c_i(x) \le 0 & i \in \mathcal{I} \\
& c_i(x) = 0 & i \in \mathcal{E}
\end{array}
\]
where at least one of the functions $f, c_i, i \in \mathcal{I} \cup \mathcal{E}$ is a black box function, meaning that we have no information about its derivatives.

We will introduce several different forms this problem can take noting those which we will discuss algorithms for.
For a more comprehensive characterization of the types constraints, consult \cite{DUMMY:typesofconstraints}.


\subsubsection{Where Problems come from}

\color{red}
laziness and parameter tuning were circled.


There are a growing number of applications for DFO. 
For example, derivative free methods can be useful when the objective is the result of a simulation that does not admit automatic differentiation.
As the popularity of complicated simulations increase, so does the demand for optimizing over black box software codes.
 which may be copyrighted.
Derivative free optimization has also been a popular method for parameter tuning, as simulations may have several parameters with unidentified relationships to their output.

\color{black}




Sometimes user laziness can preclude derivative information.
Even when it would be possible to compute derivative information, it may be prohibatively time consuming.

A trend within derivative free optimization is the permission for larger tolerances within solutions.
Their functions are frequently expensive to evaluate, so we can only ask for a small number of significant figures.
This implies slightly less concern for asymptotic convergence rates.


\subsubsection{Noise. Deterministic versus stochastic}

One branch of DFO is concerned with noisy evaluations of $S$.
Noisy functions can be categorized as either deterministic or stochastic.
Roundoff error, truncation error and finite termination errors can result in what is called deterministic noise.
This means that although a function is not evaluated accurately, the error will not change across multiple function calls with the same input.
On the other hand, stochastic noise means that each point in the domain is associated with a distribution of possible values $S$ may return.
In this paper we assume $S$ is not noisy.

\subsubsection{Types of constraints}

% If $c(x, S(x)) = c(x)$, then 
Sometimes the derivatives of $c$ are known, so the objective is the only derivative free function.
One common such case is to include bound constraints of the form $b_{L} \le x \le b_{U}$ for some $b_{L} < b_{U}$, which gives rise to Box Constrained DFO (BCDFO).
This is one of the better studied cases, with several software packages available, SPBOX, PSwarm, all NLopt algorithms except COBYLA (BOBYQA, NEWUOA, PRAXIS, Sbplx).


We are primarily interested in the case where no derivative information of $c$ is known: for example, if they are also output from a simulation used to evaluate the objective.
This means that each call to the objective gives values of the constraints as well, and vice versa.
% This produces $c(x, S(x))=c(S(x))$.

This means that we have as many points for which we know the constaints as points for which we know the objective.
This creates an interesting situation within model-based approaches which use different orders of models for the objective than the constraints.
The algorithm designer must decide how to choose a subset of these points, use a higher order model, or fit an overdetermined model.

If the constraints can be evaluated at points outside the feasible region, the constraints are called \emph{relaxable} constraints.
Some problems additionally contain ``hidden" constraints which are not explicit in the model but merely result in a notification that the objective could not be evaluated at the requested point.
For example, this can arise when simulation software fails.
This may mean that it is not possible to tell how close to a ``hidden" constraint an iterate lies.


Another area that received attention recently is that of imposing structure on $f$, and $c$.
For example, a method called Practical Optimization Using No Derivatives for sums of Squares
is developed within \cite{DUMMY:leastsquares} when $f$ takes the form of a least squares error
$f(x) = \frac 1 2 \|F(x)\|^2$ over bound constraints where $F$ is a nonlinear, vector valued function.

%Many DFO methods simply let $f(x,S(x)) = S(x)$.



\subsubsection{Importance of Derivatives}

The lack of derivative information means that DFO methods are at a disadvantage when compared to their counterparts in nonlinear optimization.
First and second derivative information is explicit in algorithms with quadratic convergence such as Newton's method.
They are also present in conditions for convergence results such as Wolf's, Armijo or Goldstien for line search methods.
Additionally, stopping criteria usually involve a criticality test involving derivatives.
When derivatives are known, they should be used.
% For this reason, it is desirable for $n$, the dimension of $x$, to be small.

\subsection{}

\section{Background}
\subsection{Derivative Free Algorithm classes}
\subsubsection{Automatic Differentiation}

When $S$ is the result of a simulation for which the source code is available, one convenient approach is to perform automatic differentiation.
Although derivatives of complicated expressions resulting from code structure are difficult to work with on paper, the rules of differentiation can be applied algorithmically.
However, the nature of the code or problem can make this very difficult: for example with combinatorial problems that rely heavily on if statements.

\subsubsection{Direct search}


Another approach is to use direct search methods that do not explicitly estimate derivative information but evaluate the objective on a pattern or other structure to find a descent direction.
Examples of this include Coordinate descent, implicit filtering and other pattern based search methods.
One of the most popular direct search method is Nelder Mead, which is implemented in fminsearch in Matlab.
It remains popular although it is proven to not converge in pathological cases unless modifications are made.

These methods can be robust in that they are insensitive to scaling and often converge to a local minimum even when assumptions such as smoothness or continuity are violated.
However, they ignore potentially helpful information because they do not use derivative information provided through the function evaluations.
This means that they can also lack fast convergence rates.

%\color{red}
%(0th derivative)
%\color{black}

\subsubsection{Finite difference methods}

Finite difference methods can be used to approximate the derivative of a function $f$.
One common approximation called the symmetric difference is given by $\nabla f(x) \approx (\frac{f(x+he_i) - f(x-he_i)}{2h})_i$ for some small $h$ where $e_i = (0,\ldots, 0, 1, 0, \ldots, 0)^T \quad \forall \; 1 \le i \le n$ is the unit vector with $1$ in its $i$th component.
%This may work well, but can have issues with unlucky iterates (CITE).
However, the number of function evaluations tends to grow large with the dimension and number of iterations the algorithm performs.
This is because derivative information is only gathered near the current iterate when $h$ is small, which is required for accurate derivative calculations.
Because of the large number of function evaluations required for finite difference schemes, it may preferable to spread sample points out over the entire region where we may expect to step.
Also, function evaluations may be subject to noise, making the finite difference approximations of derivative problematic.

\subsubsection{Model based methods}

In this paper, we are concerned with model based methods.
These typically minimize a model function that only approximates the objective and constraints.
The model functions are chosen to accurately represent the original function, but allow for derivative information to be calculated easily.
They work by evaluating functions on a set of sample points to construct local models of the functions.

This allows the algorithm to minimize these easier model functions over a trust region, rather than working with the original function.
When derivatives are given in the original function, model-based methods can also be used to approximate derivative information.
We will see several examples in what follows.


%\color{red}
%Kriging seems to be another popular model function, but I usually see it used within global optimization.
%\color{black}




\paragraph{Interpolation/regression methods}

Within Interpolation methods, we construct our model by regressing basis functions onto a set of sampled points.
For example, given a function $f(x) : \mathbb R^n \to \mathbb R$ we can use a set of basis functions $\phi_i : \mathbb R^n \to \mathbb R \quad \forall 1 \le i \le d_1$ to construct a model function $m(x) = \sum_{i=1}^{d_1} \lambda_i \phi_i(x)$ approximating $f(x)$ by selecting appropriate $\lambda_i \in \mathbb R$.
This is done by choosing a set of sample points
$Y = \{y^1, y^2, \ldots, y^{d_2}\}$,
evaluating $f = (f_1 = f(y^1), f_2 = f(y^2), \ldots, f_d = f(y^{d_2}))^T$ and forcing model agreement with the original function $f(x)$ by ensuring

\begin{equation}
\label{reg}
\begin{bmatrix}
    \phi_1(y^1)      & \phi_2(y^1)       & \ldots & \phi_{d_1}(y^1)      \\
    \phi_1(y^2)      & \phi_2(y^2)       & \dots  & \phi_{d_1}(y^2)      \\
                     &                   & \vdots &                      \\
    \phi_1(y^{d_2})  & \phi_2(y^{d_2})   & \ldots & \phi_{d_1}(y^{d_2})
\end{bmatrix}
\begin{bmatrix}
    \lambda_1      \\
    \lambda_2      \\
    \vdots         \\            
    \lambda_{d_1}
\end{bmatrix}
\approx
\begin{bmatrix}
    f_1      \\
    f_2      \\
    \vdots         \\            
    f_{d_2}
\end{bmatrix}.
\end{equation}

When $d_1 = d_2$ this is called interpolation and equality is desired within \ref{reg}.
When $d_1 < d_2$ this is called underdetermined interpolation.
This can be handled by requesting and a minimum norm solution.
Finally, when $d_1 > d_2$ this is called regression and only a least squares solution can be requested.
Note that in practice, the set $Y$ is shifted and scaled.

\paragraph{Basis functions}

The choice of model functions $\phi_i$ can have some affect on the convergence rate, as Powell showed in \cite{DUMMY:PowellRadialBasis}.
One common choice of basis functions is the Lagrange polynomials, in which we select polynomials satisfying $\phi_{i}(y^j) = \delta_{ij}$, the kroneker delta function.
This reduces the matrix within \ref{reg} to an identity matrix.
Lagrange polynomials of order $p$ can be computed by starting with the monomial basis $\prod_{i=1}^{n} x_i^{n_i}$ for all choices of $0 \le n_i \le p$ with $\sum_{i=1}^n n_i \le p$ and inverting the corresponding Vandermonde matrix.

Newton's Fundamental polynomials are also used, and follow a similar pattern.
However, they maintain different orders of polynomials within the basis:
a single constant value, a set of $n+1$ linear polynomials,
$n + {n \choose 2}$ quadratic functions, and more for higher order polynomials.
Radial basis functions may have some intuitive advantage because the algorithm makes claims about the accuracy of the function over a trust region.


Model functions are usually chosen to be fully linear or fully quadratic: terms describing how the model's error grows as a function of the trust region radius.

\paragraph{Sampling issues}

These methods have issues with poor sampling choices.
The two most important aspects of the sample points are their geometry and proximity.

\subparagraph{Geometry}

For geometry, regression based methods require that the set the function is evaluated at must be $\Lambda$-poised for a fixed constant $\Lambda$.
If the set is $\Lambda$-poised for a certain trust region radius, then the maximum absolute value of any Lagrange polynomial over the trust region is one.
This ensures that the Vandermonde matrix used to find the coefficients used to express the model function in terms of a basis of Lagrange polynomials is well conditioned.
Although we do not go into the details here, problems become apparent when comparing the Lagrange polynomials associated with a poised set with those of an ill poised set.

Within the first set of pictures below we see the set of quadratic Lagrange polynomials on the interval $[-1,1]$.
The maximum value of these polynomials over the trust region is simply $1$.
However, if we use sample points $\{-1, .9, 1\}$ instead of the points $\{-1, 0, 1\}$,
we find the second set of polynomials.

\includegraphics[width=200px]{poised.png}
\includegraphics[width=200px]{illpoised.png}

If we use these to approximate $3 + \tan^{-1}(x)$, the first basis functions do not vary far from the objective value over the trust region, and the maximum difference between the function and the model function is $0.0711$.
However, the second basis functions jump far away from the actual function, and the maximum difference between the model function and actual function is 
$0.1817$.

\includegraphics[width=200px]{poised_approx.png}
\includegraphics[width=200px]{illpoised_approx.png}


\subparagraph{Proximity}

Proximity refers to the trust region radius.
The trust region must go to zero if we are to be sure that we have reached a critical point.
In general, the smaller the trust region, the closer to linear or quadratic the original function will look.
This is because the model's error term given by Taylor's expansion is proportional to the trust region radius.

%\color{red} Within noisy optimization, this gives rise to several more problems. If you think there is something else about proximity that I should mention, please let me know. \color{black}

\subsubsection{Model-based, Trust Region Methods}

The overall description of the trust region framework is that a set of poised points are chosen for some radius $\Delta>0$ about the current iterate.
The objective and constraints are then evaluated at these points to construct a model function as a linear combination of some set of basis functions.
Next, the model is minimized over this trust region and the argument minimum becomes the trial point.
The objective is evaluated at the trial point and a measure of reduction $\rho$ is computed.
If $\rho$ implies that sufficient reduction has been made and that the model approximates the function well, the trial point is accepted as the new iterate.
Otherwise, the trust region is reduced to increase model accuracy.


For unconstrained optimization, the algorithmic framework can be described with these steps:

\begin{enumerate}
	\item Create a model function $m_k(x)$.
	\begin {itemize}
		\item In classical trust region methods, the following quadratic model function is used:
		\[
		m_k(x) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x-x^{(k)}) + \frac 1 2 (x-x^{(k)})^T\nabla^2f(x^{(k)})(x-x^{(k)})
		\]
	\end{itemize}
	\begin{itemize}
		\item $\nabla f(x^{(k)})$ and $\nabla^2 f(x^{(k)})$ must be approximated
		\item Geometric properties of the sample set that must be satisfied
	\end{itemize}
	
	\item If $\nabla m_k(x) < \tau$ stop, where $\tau$ is some tolerance
	
	\item Solve the Trust region subproblem: $s^{(k)} = \argmin_{s\in B_{x^{(k)}}( \Delta_k)} m_k(x^{(k)} + s)$
	\begin {itemize}
		\item $B_{x^{(k)}}(\Delta_k) = \{ x \in \mathbb{R}^n : \| x - x^{(k)} \le \Delta_k \}$ 
		is the ball of radius $\Delta_k$ centered at $x^{(k)}$
	\end{itemize}
	
	\item Test for improvement
	\begin{itemize}
		\item Compute
\begin{equation}
\label{rho}
\rho_k = \frac{f(x^{(k)}) - f(x^{(k)}+s^{(k)})}{m_k(x^{(k)}) - m_k(x^{(k)}+s^{(k)})}
\end{equation}
which measures the actual improvement over predicted improvement
		\item If $\rho$ is small, $x^{(k+1)}=x^{(k)}$ (reject) and decrease radius
		\item If $\rho$ is intermediate, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and decrease radius
		\item If $\rho$ is large, $x^{(k+1)}=x^{(k)}+s^{(k)}$ (accept) and either increase the radius or decrease if $\nabla m_k(x_k)$ is small
	\end{itemize}
	
	\item Go to step 1
\end{enumerate}

Our goal is generalize this framework to handle constraints, where we must reduce constraint violation while ensuring the accuracy of the models of the constraints.


\subsubsection{Trust region versus Linesearch} \label{linevsmodel}

Within derivative free optimization, we can ensure the accuracy of our model function by sampling points over a small enough trust region.
However, reducing the trust region implies more points must be evaluated.
Linesearch methods rely on the the ability to calculate a descent direction that will be accurate in a small enough region around the current iterate:
small enough that the trust region must be reduced to ensure the model's accuracy.

This means trust region framework fits into derivative free optimization more naturally than line search methods.
Not only do the trust regions arise naturally, but many line search algorithms exploit how much easier it is to find a descent direction than solve a trust region subproblem.
However, in derivative free optimization, solving a costly trust region subproblem is acceptable if it allows us to avoid even more expensive function evaluations.


% We began with a line search filter method, however we found that this had several drawbacks:
% \begin{itemize}
% \item Line search algorithms exploit how much easier finding a descent direction is than solving the trust region subproblem, but this saved computation is not as useful in DFO as here it is function evaluations that we avoid
% \item As the algorithm backtracks on the step length, the trust region must be reduced, as the models are accurate over a region rather than at a single point
% \end{itemize}



\subsubsection{Literature Background}

\paragraph{Derivative free methods}

Within  \cite{DUMMY:intro_book} derivative-free methods are developed in detail.
This is the first text book devoted to derivative free optimization.
It contains a good explanation of ensuring geometry of the current set with poisedness for unconstrained problems and also covers other derivative-free methods including direct-search and line search.

A good review of derivative free algorithms and software libraries can be found in \cite{DUMMY:review}.
This compares several software libraries, and reviews the development of derivative free optimization since it started.
Another recent review can be found in \cite{DUMMY:review2}.

Within \cite{DUMMY:linesearch_global} and \cite{DUMMY:linesearch_local} Biegler uses a filter method to ensure global convergence within a line search framework.

\section{Experiments}
\subsection{SQP Filter method}

We will be considering problems of the form


\begin{center}
\begin{align}
\label{problem}
\min_x & \quad f(x) \\
  c_i(x) \le 0   & \quad \forall i \in \mathcal {I} \nonumber \\
  c_i(x)  = 0    & \quad \forall i \in \mathcal {E} \nonumber
\end{align}
\end{center}
where $f : \mathbb R^n \to \mathbb R$, and each $c_i : \mathbb{R} \to \mathbb{R}$.
It will be convenient to write
$c_{\mathcal {I}}(x) = (c_1(x), c_2(x), \ldots, c_{|\mathcal{I}|})^T$,
$c_{\mathcal {E}}(x) = (c_{|\mathcal{I}|+1}(x), c_{|\mathcal{I}|+2}(x), \ldots, c_{|\mathcal{I}| + |\mathcal{E}|})^T$ and
$c(x) = (c_{\mathcal{I}}^T(x), c_{\mathcal{E}}^T(x))^T$.


We work within a trust region, sequential quadratic programming framework with interpolating model functions that uses a filter method introduced by Gould and Toint \cite{original_filter}.
For our algorithm, we assume that all functions are derivative-free: all functions are evaluated by a single call to a black box function:
$S(x) = (f(x), c_{\mathcal {I}}(x)^T, c_{\mathcal {E}}(x)^T)^T$.

We also assume that we are not able to  evaluate points outside the feasible region.

We first compute an interpolation set poised for regressing a set of model functions, which we choose to be quadratic functions.
Although we have enough sample points to construct a quadratic model of the constraints, we only construct linear models to avoid the complexity of Quadratically Constrained Quadratic Programming which is NP-hard.

\subsubsection{Model functions}

At an iteration $k$, we first construct a model function $m_f^k(x)$ that we use to approximate the first and second derivatives of $f(x)$.
We also construct $m_{\mathcal{I}}^k(x)$ and $m_{\mathcal{E}}^k(x)$ to approximate the first derivatives of $c(x)$.
Namely, at an iterate $x^k$, we let $f^k = f(x^k)$, $g^k = \nabla m_f^k(x^k)$. %be the gradient of the objective's model at $x^k$.
We also define the $c_{{\mathcal{I}}}^k = c^k_{\mathcal{I}}(x^k)$, 
$A_{{\mathcal{I}}}^k = \nabla m_{\mathcal{I}}^k(x^k)$,
$c_{{\mathcal{E}}}^k = c_{\mathcal{E}}(x^k)$, and
$A_{{\mathcal{E}}}^k = \nabla m_{\mathcal{E}}^k(x^k)$.

We will also need to compute the Hessian of the Lagrangian for \ref{problem}.
The Lagrangian is defined as 
 $L(x, \mu, \lambda) =
 f(x)  
 + \sum_{i \in {\mathcal{I}}} \mu_i     c_i(x)
 + \sum_{i \in {\mathcal{E}}} \lambda_i c_i(x)$.
To compute its second derivative, we let $A^k$ and $c^k$ contain $A_{\mathcal{E}}^k$ and $c_{\mathcal{E}}^k$ as well as any rows of $A_{{\mathcal{I}}}^k$ and $c_{\mathcal{E}}^k$ corresponding
to active constraints at $x^k$.
The set of active constraints $\mathcal A \subseteq \mathcal I \cup \mathcal E$ includes $\mathcal E$ and any $i \in \mathcal I$ for which $c_i(x^k) \ge 0$.
We then solve the system
\begin{align*}
\nabla^2m_k(x_k) & d + {A^k}^T\lambda & = g^k \\
A^k              & d                & = c^k
\end{align*}
for $\lambda$ and compute
\[
H^k = \nabla^2 m_k(x^k) 
+ \sum_{i \in \mathcal E} \lambda_i \nabla^2 m_{g,i}^k(x^k) 
+ \sum_{i \in \mathcal A \backslash \mathcal E} \lambda_i \nabla^2 m_{h,i}^k(x^k).
\]

With these definitions, we define the quadratic subproblem as

\begin{align*}
\argmin_s  f(x^k+s) = f^k + (g^k)^Ts + \frac 1 2 (s^k)^T H^ks^k  &\\
     c_{{\mathcal{I}}}^k + A_{{\mathcal{I}}}^ks       &\le \; 0  \\
s.t. \hspace{1cm} c_{{\mathcal{E}}}^k + A_{{\mathcal{E}}}^ks           &=\; 0 \\
     \| s \|                      &\le \; \Delta_k.
\end{align*}



\subsubsection{Criticality Measure}

In order to construct stopping criteria, we introduce a criticality measure $\chi$ which goes to zero as the iterates approach a first order critical point.
Once this has reached small enough threshold, and the trust region is small enough, we can terminate the algorithm.

That is, if $x^{\star}$ is a first order critical point satisfying

\begin{align*}
\nabla f(x^{\star}) + \sum_{i\in\mathcal I} \lambda_i \nabla c_i(x^{\star}) + \sum_{i\in \mathcal E} \mu_i \nabla c_i(x^{\star})  = 0 \\
c(x^{\star})_i \mu_i = 0 & \quad \forall i\in\mathcal I \\
c(x^{\star}) \le 0 & \quad \forall i\in\mathcal I\\
c(x^{\star}) = 0 & \quad \forall i\in\mathcal E
\end{align*}
for some $\lambda_i \in \mathbb R$, and $\mu_i \ge 0$, then we need $\lim_{x\to x^{\star}} \chi(x) = 0$.

This suggests the following definition:
\begin{align}
\label{critical}
\chi & = & |\min_t \langle g_k + H_kn_k, t\rangle| \\
& A_{\mathcal {E}}t &=& \; 0 \\
& c_{\mathcal {I}} + A_{\mathcal {I}}t &\le& \; 0 \\
& \| t \| &\le& \; 1
\end{align}

Notice that this must be computed after the normal step $n^k$ has been computed.


\subsubsection{Sufficient reduction of model}
It is not enough to simply ask for decrease in the objective every iteration.
We also require \emph{sufficient} reduction, so that any accumulation point of the sequences of iterates is feasible.
To this end, we introduce constants $0 < \kappa_{\theta} < 1$ and $\psi > \frac{1}{1+\mu}$ and require

\begin{equation}
\label{predicted_decrease}
m_k(x_k) - m_k(x_k+s_k) \ge \kappa_{\theta} \theta_k^{\psi}.
\end{equation}

%\color{red}
%Although this only references the model functions, we ensure accuracy by computing $\rho$.
%\color{black}

%\subsubsection{Compatibility}

\newpage
\subsection{The Algorithm}

With these tools, we can now describe the algorithm.
The overall steps are as follows:

\underline{\hspace{20cm}}
\FloatBarrier
\begin{enumerate}
\item \textbf{Initialize} each constant introduced so far

\item \textbf{Compute model functions} $m_f$, $m_g$, $m_h$, the active constraints $\mathcal A$, and the Hessian of the Lagrangian $H_k$

\item \textbf{Compute the normal step} $n_k$ according to \ref{normal}

If the feasible region of \ref{normal} is empty or \ref{compat} is not satisfied, then 
\begin{itemize}
\item Add $(\theta_k, f_k)$ to the filter
\item Call the feasibility restoration routine
\item Increment $k$
\item Go to step 2
\end{itemize}

\item \textbf{Compute criticality measure} $\chi$ from \ref{critical}
\begin{itemize}
\item If $\chi$ is small, but the trust region radius is larger than the tolerance, then decrease the trust region, increment $k$ and return to step 2
\item Otherwise, if $\chi$ and the trust region radius is smaller than the tolerance, return success
\item If $\chi$ is zero, then return success
\end{itemize}


\item \textbf{Compute tangential step} $t_k$ according to \ref{tangent}

Define $s_k = n_k + t_k$.

\item Check for \textbf{sufficient reduction} in the model function

If \ref{predicted_decrease} is satisfied
\begin{itemize}
\item Add $(\theta_k, f_k)$ to the filter
\item Increment $k$
\item Go to step 2
\end{itemize}

\item \textbf{Evaluate} the function and constraints at the trial point $x_k + s_k$

If the new point is not acceptable to the filter according to either \ref{acceptable1} or \ref{acceptable2}, then
\begin{itemize}
\item Add $(\theta_k, f_k)$ to the filter
\item Increment $k$
\item Go to step 2
\end{itemize}

\item \textbf{Compute $\rho$} according to \ref{rho}
\begin{itemize}
\item If $\rho \le \gamma_1$ is small, then decrease the trust region radius and go to step 2
\item If $\gamma_1 < \rho \le \gamma_2$ is intermediate, then accept the trial point $x_{k+1} = x_{k} + s_k$.
\item If $\rho > \gamma_2$, and $\chi$ is small, then decrease the trust region radius.
\item If $\rho > \gamma_2$ is large, and $\chi$ is large, then increase the trust region radius.
\end{itemize}

\item increment $k$ and go to step 2
\end{enumerate}
\FloatBarrier

\underline{\hspace{20cm}}


\bibliography{proposal}
\bibliographystyle{ieeetr}

\end{document}

